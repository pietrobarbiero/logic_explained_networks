{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sympy import simplify_logic\n",
    "\n",
    "from deep_logic.utils.relunn import get_reduced_model, prune_features\n",
    "from deep_logic import fol\n",
    "from deep_logic.utils.base import collect_parameters\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_expression_matrix = pd.read_csv('w_1/data_0.csv', index_col=None, header=None)\n",
    "labels = pd.read_csv('w_1/tempLabels_W-1.csv', index_col=None, header=None)\n",
    "genes = pd.read_csv('w_1/features_0.csv', index_col=None, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>28392</th>\n",
       "      <th>28393</th>\n",
       "      <th>28394</th>\n",
       "      <th>28395</th>\n",
       "      <th>28396</th>\n",
       "      <th>28397</th>\n",
       "      <th>28398</th>\n",
       "      <th>28399</th>\n",
       "      <th>28400</th>\n",
       "      <th>28401</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.622486</td>\n",
       "      <td>11.162004</td>\n",
       "      <td>3.320000</td>\n",
       "      <td>3.320000</td>\n",
       "      <td>3.320000</td>\n",
       "      <td>12.788433</td>\n",
       "      <td>6.143456</td>\n",
       "      <td>3.320000</td>\n",
       "      <td>4.876620</td>\n",
       "      <td>3.320000</td>\n",
       "      <td>...</td>\n",
       "      <td>3.32</td>\n",
       "      <td>3.320000</td>\n",
       "      <td>3.885589</td>\n",
       "      <td>3.914260</td>\n",
       "      <td>3.320000</td>\n",
       "      <td>3.32</td>\n",
       "      <td>3.320000</td>\n",
       "      <td>4.465420</td>\n",
       "      <td>3.320000</td>\n",
       "      <td>4.973620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14.398743</td>\n",
       "      <td>11.000080</td>\n",
       "      <td>3.320000</td>\n",
       "      <td>3.320000</td>\n",
       "      <td>3.320000</td>\n",
       "      <td>12.845914</td>\n",
       "      <td>6.147482</td>\n",
       "      <td>3.320000</td>\n",
       "      <td>4.484223</td>\n",
       "      <td>3.320000</td>\n",
       "      <td>...</td>\n",
       "      <td>3.32</td>\n",
       "      <td>3.575025</td>\n",
       "      <td>4.236519</td>\n",
       "      <td>4.047825</td>\n",
       "      <td>3.320000</td>\n",
       "      <td>3.32</td>\n",
       "      <td>4.176269</td>\n",
       "      <td>3.320000</td>\n",
       "      <td>4.553796</td>\n",
       "      <td>4.967418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14.692079</td>\n",
       "      <td>11.100175</td>\n",
       "      <td>3.320000</td>\n",
       "      <td>4.171535</td>\n",
       "      <td>3.320000</td>\n",
       "      <td>12.712544</td>\n",
       "      <td>5.583210</td>\n",
       "      <td>3.320000</td>\n",
       "      <td>3.478171</td>\n",
       "      <td>3.320000</td>\n",
       "      <td>...</td>\n",
       "      <td>3.32</td>\n",
       "      <td>3.320000</td>\n",
       "      <td>3.992331</td>\n",
       "      <td>4.865538</td>\n",
       "      <td>3.320000</td>\n",
       "      <td>3.32</td>\n",
       "      <td>3.488281</td>\n",
       "      <td>3.406285</td>\n",
       "      <td>3.320000</td>\n",
       "      <td>6.676063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.613382</td>\n",
       "      <td>11.023209</td>\n",
       "      <td>3.320000</td>\n",
       "      <td>3.320000</td>\n",
       "      <td>3.320000</td>\n",
       "      <td>12.750496</td>\n",
       "      <td>5.688023</td>\n",
       "      <td>3.320000</td>\n",
       "      <td>4.464426</td>\n",
       "      <td>3.320000</td>\n",
       "      <td>...</td>\n",
       "      <td>3.32</td>\n",
       "      <td>3.855643</td>\n",
       "      <td>3.320000</td>\n",
       "      <td>4.905350</td>\n",
       "      <td>3.320000</td>\n",
       "      <td>3.32</td>\n",
       "      <td>4.158393</td>\n",
       "      <td>4.433457</td>\n",
       "      <td>3.874214</td>\n",
       "      <td>5.981160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14.482065</td>\n",
       "      <td>10.989851</td>\n",
       "      <td>3.320000</td>\n",
       "      <td>3.992726</td>\n",
       "      <td>4.574745</td>\n",
       "      <td>12.878702</td>\n",
       "      <td>6.195418</td>\n",
       "      <td>4.177962</td>\n",
       "      <td>3.872567</td>\n",
       "      <td>3.320000</td>\n",
       "      <td>...</td>\n",
       "      <td>3.32</td>\n",
       "      <td>3.320000</td>\n",
       "      <td>3.320000</td>\n",
       "      <td>4.879493</td>\n",
       "      <td>3.320000</td>\n",
       "      <td>3.32</td>\n",
       "      <td>4.571869</td>\n",
       "      <td>3.320000</td>\n",
       "      <td>4.982136</td>\n",
       "      <td>6.145585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>14.565031</td>\n",
       "      <td>11.699843</td>\n",
       "      <td>3.320000</td>\n",
       "      <td>3.320000</td>\n",
       "      <td>3.320000</td>\n",
       "      <td>12.789212</td>\n",
       "      <td>6.504027</td>\n",
       "      <td>3.320000</td>\n",
       "      <td>3.320000</td>\n",
       "      <td>6.182912</td>\n",
       "      <td>...</td>\n",
       "      <td>3.32</td>\n",
       "      <td>3.320000</td>\n",
       "      <td>3.320000</td>\n",
       "      <td>4.338456</td>\n",
       "      <td>3.771718</td>\n",
       "      <td>3.32</td>\n",
       "      <td>3.320000</td>\n",
       "      <td>3.320000</td>\n",
       "      <td>3.320000</td>\n",
       "      <td>6.100644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>14.624502</td>\n",
       "      <td>11.918757</td>\n",
       "      <td>3.320000</td>\n",
       "      <td>4.292406</td>\n",
       "      <td>3.430485</td>\n",
       "      <td>10.728709</td>\n",
       "      <td>6.197159</td>\n",
       "      <td>3.320000</td>\n",
       "      <td>4.089918</td>\n",
       "      <td>5.201608</td>\n",
       "      <td>...</td>\n",
       "      <td>3.32</td>\n",
       "      <td>3.320000</td>\n",
       "      <td>3.320000</td>\n",
       "      <td>4.790134</td>\n",
       "      <td>3.320000</td>\n",
       "      <td>3.32</td>\n",
       "      <td>4.985474</td>\n",
       "      <td>4.444057</td>\n",
       "      <td>3.580523</td>\n",
       "      <td>6.301926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>14.585190</td>\n",
       "      <td>11.090112</td>\n",
       "      <td>3.320000</td>\n",
       "      <td>3.674768</td>\n",
       "      <td>3.320000</td>\n",
       "      <td>12.877485</td>\n",
       "      <td>6.326960</td>\n",
       "      <td>3.320000</td>\n",
       "      <td>3.320000</td>\n",
       "      <td>3.320000</td>\n",
       "      <td>...</td>\n",
       "      <td>3.32</td>\n",
       "      <td>3.320000</td>\n",
       "      <td>3.320000</td>\n",
       "      <td>4.547342</td>\n",
       "      <td>3.320000</td>\n",
       "      <td>3.32</td>\n",
       "      <td>4.064473</td>\n",
       "      <td>3.320000</td>\n",
       "      <td>4.254152</td>\n",
       "      <td>5.964505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>14.449554</td>\n",
       "      <td>10.805855</td>\n",
       "      <td>3.320000</td>\n",
       "      <td>3.320000</td>\n",
       "      <td>3.320000</td>\n",
       "      <td>12.660038</td>\n",
       "      <td>6.261395</td>\n",
       "      <td>3.320000</td>\n",
       "      <td>4.125096</td>\n",
       "      <td>3.320000</td>\n",
       "      <td>...</td>\n",
       "      <td>3.32</td>\n",
       "      <td>3.784260</td>\n",
       "      <td>3.644823</td>\n",
       "      <td>4.546974</td>\n",
       "      <td>3.427441</td>\n",
       "      <td>3.32</td>\n",
       "      <td>4.666265</td>\n",
       "      <td>3.888525</td>\n",
       "      <td>3.765754</td>\n",
       "      <td>5.452018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>14.439020</td>\n",
       "      <td>11.080826</td>\n",
       "      <td>4.152651</td>\n",
       "      <td>3.320000</td>\n",
       "      <td>3.320000</td>\n",
       "      <td>12.187504</td>\n",
       "      <td>6.154358</td>\n",
       "      <td>3.320000</td>\n",
       "      <td>4.247009</td>\n",
       "      <td>3.320000</td>\n",
       "      <td>...</td>\n",
       "      <td>3.32</td>\n",
       "      <td>3.671102</td>\n",
       "      <td>3.519157</td>\n",
       "      <td>4.771458</td>\n",
       "      <td>3.320000</td>\n",
       "      <td>3.32</td>\n",
       "      <td>5.115644</td>\n",
       "      <td>3.320000</td>\n",
       "      <td>3.410361</td>\n",
       "      <td>6.449961</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61 rows × 28402 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0          1         2         3         4          5         6      \\\n",
       "0   14.622486  11.162004  3.320000  3.320000  3.320000  12.788433  6.143456   \n",
       "1   14.398743  11.000080  3.320000  3.320000  3.320000  12.845914  6.147482   \n",
       "2   14.692079  11.100175  3.320000  4.171535  3.320000  12.712544  5.583210   \n",
       "3   14.613382  11.023209  3.320000  3.320000  3.320000  12.750496  5.688023   \n",
       "4   14.482065  10.989851  3.320000  3.992726  4.574745  12.878702  6.195418   \n",
       "..        ...        ...       ...       ...       ...        ...       ...   \n",
       "56  14.565031  11.699843  3.320000  3.320000  3.320000  12.789212  6.504027   \n",
       "57  14.624502  11.918757  3.320000  4.292406  3.430485  10.728709  6.197159   \n",
       "58  14.585190  11.090112  3.320000  3.674768  3.320000  12.877485  6.326960   \n",
       "59  14.449554  10.805855  3.320000  3.320000  3.320000  12.660038  6.261395   \n",
       "60  14.439020  11.080826  4.152651  3.320000  3.320000  12.187504  6.154358   \n",
       "\n",
       "       7         8         9      ...  28392     28393     28394     28395  \\\n",
       "0   3.320000  4.876620  3.320000  ...   3.32  3.320000  3.885589  3.914260   \n",
       "1   3.320000  4.484223  3.320000  ...   3.32  3.575025  4.236519  4.047825   \n",
       "2   3.320000  3.478171  3.320000  ...   3.32  3.320000  3.992331  4.865538   \n",
       "3   3.320000  4.464426  3.320000  ...   3.32  3.855643  3.320000  4.905350   \n",
       "4   4.177962  3.872567  3.320000  ...   3.32  3.320000  3.320000  4.879493   \n",
       "..       ...       ...       ...  ...    ...       ...       ...       ...   \n",
       "56  3.320000  3.320000  6.182912  ...   3.32  3.320000  3.320000  4.338456   \n",
       "57  3.320000  4.089918  5.201608  ...   3.32  3.320000  3.320000  4.790134   \n",
       "58  3.320000  3.320000  3.320000  ...   3.32  3.320000  3.320000  4.547342   \n",
       "59  3.320000  4.125096  3.320000  ...   3.32  3.784260  3.644823  4.546974   \n",
       "60  3.320000  4.247009  3.320000  ...   3.32  3.671102  3.519157  4.771458   \n",
       "\n",
       "       28396  28397     28398     28399     28400     28401  \n",
       "0   3.320000   3.32  3.320000  4.465420  3.320000  4.973620  \n",
       "1   3.320000   3.32  4.176269  3.320000  4.553796  4.967418  \n",
       "2   3.320000   3.32  3.488281  3.406285  3.320000  6.676063  \n",
       "3   3.320000   3.32  4.158393  4.433457  3.874214  5.981160  \n",
       "4   3.320000   3.32  4.571869  3.320000  4.982136  6.145585  \n",
       "..       ...    ...       ...       ...       ...       ...  \n",
       "56  3.771718   3.32  3.320000  3.320000  3.320000  6.100644  \n",
       "57  3.320000   3.32  4.985474  4.444057  3.580523  6.301926  \n",
       "58  3.320000   3.32  4.064473  3.320000  4.254152  5.964505  \n",
       "59  3.427441   3.32  4.666265  3.888525  3.765754  5.452018  \n",
       "60  3.320000   3.32  5.115644  3.320000  3.410361  6.449961  \n",
       "\n",
       "[61 rows x 28402 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gene_expression_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>diagnosis: healthy control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>diagnosis: healthy control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>diagnosis: healthy control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>diagnosis: healthy control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>diagnosis: healthy control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>omalizumab responder status: Responder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>omalizumab responder status: Responder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>omalizumab responder status: Responder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>omalizumab responder status: Responder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>omalizumab responder status: Responder</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         0\n",
       "0               diagnosis: healthy control\n",
       "1               diagnosis: healthy control\n",
       "2               diagnosis: healthy control\n",
       "3               diagnosis: healthy control\n",
       "4               diagnosis: healthy control\n",
       "..                                     ...\n",
       "56  omalizumab responder status: Responder\n",
       "57  omalizumab responder status: Responder\n",
       "58  omalizumab responder status: Responder\n",
       "59  omalizumab responder status: Responder\n",
       "60  omalizumab responder status: Responder\n",
       "\n",
       "[61 rows x 1 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\pietr\\anaconda3\\envs\\deep-logic\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40, 28402])\n",
      "torch.Size([40, 1])\n"
     ]
    }
   ],
   "source": [
    "encoder = LabelEncoder()\n",
    "labels_encoded = encoder.fit_transform(labels.values)\n",
    "labels_encoded_noncontrols = labels_encoded[labels_encoded!=0] - 1\n",
    "\n",
    "data_controls = gene_expression_matrix[labels_encoded==0]\n",
    "data = gene_expression_matrix[labels_encoded!=0]\n",
    "\n",
    "gene_signature = data_controls.mean(axis=0)\n",
    "data_scaled = data - gene_signature\n",
    "\n",
    "scaler = MinMaxScaler((0, 1))\n",
    "scaler.fit(data_scaled)\n",
    "data_normalized = scaler.transform(data_scaled)\n",
    "\n",
    "x = torch.FloatTensor(data_normalized)\n",
    "y = torch.FloatTensor(labels_encoded_noncontrols).unsqueeze(1)\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train accuracy: 0.2250\n",
      "Epoch 1000: train accuracy: 0.7500\n",
      "Epoch 2000: train accuracy: 0.7500\n",
      "Epoch 3000: train accuracy: 0.7500\n",
      "Epoch 4000: train accuracy: 0.9500\n",
      "Epoch 5000: train accuracy: 0.9750\n",
      "Epoch 6000: train accuracy: 1.0000\n",
      "Epoch 7000: train accuracy: 1.0000\n",
      "Epoch 8000: train accuracy: 1.0000\n",
      "Epoch 9000: train accuracy: 1.0000\n",
      "Epoch 10000: train accuracy: 1.0000\n",
      "Epoch 11000: train accuracy: 1.0000\n",
      "Epoch 12000: train accuracy: 1.0000\n",
      "Epoch 13000: train accuracy: 1.0000\n",
      "Epoch 14000: train accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "x = x.to(device)\n",
    "y = y.to(device)\n",
    "\n",
    "layers = [\n",
    "    torch.nn.Linear(x.size(1), 10),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(10, 5),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(5, 1),\n",
    "    torch.nn.Sigmoid(),\n",
    "]\n",
    "model = torch.nn.Sequential(*layers).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "model.train()\n",
    "need_pruning = True\n",
    "for epoch in range(15000):\n",
    "    # forward pass\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model(x)\n",
    "    # Compute Loss\n",
    "    loss = torch.nn.functional.mse_loss(y_pred, y)\n",
    "\n",
    "    for module in model.children():\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            loss += 0.01 * torch.norm(module.weight, 1)\n",
    "            loss += 0.01 * torch.norm(module.bias, 1)\n",
    "\n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # compute accuracy\n",
    "    if epoch % 1000 == 0:\n",
    "        y_pred_d = (y_pred > 0.5)\n",
    "        accuracy = (y_pred_d.eq(y).sum(dim=1) == y.size(1)).sum().item() / y.size(0)\n",
    "        print(f'Epoch {epoch}: train accuracy: {accuracy:.4f}')\n",
    "        \n",
    "    if epoch > 8000 and need_pruning and epoch % 1000 == 0:\n",
    "        prune_features(model, 1, device)\n",
    "        need_pruning = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input 1\n",
      "\tx=[0.94 0.31 0.   ... 0.26 0.8  0.68]\n",
      "\ty=[0.]\n",
      "\ty=[0.99]\n",
      "\tExplanation: ~feature0000006749 & ~feature0000010382 & ~feature0000013464 & feature0000013710 & feature0000022010 & ~feature0000025379 & ~feature0000026329\n",
      "\n",
      "Input 2\n",
      "\tx=[0.84 0.71 0.   ... 0.26 0.   0.62]\n",
      "\ty=[0.]\n",
      "\ty=[0.99]\n",
      "\tExplanation: ~feature0000026329\n",
      "\n",
      "Input 3\n",
      "\tx=[0.62 0.36 0.   ... 0.   0.   0.71]\n",
      "\ty=[0.]\n",
      "\ty=[0.99]\n",
      "\tExplanation: ~feature0000006749 & ~feature0000010382 & ~feature0000013464 & ~feature0000013710 & ~feature0000022010 & ~feature0000025379 & feature0000026329\n",
      "\n",
      "Input 4\n",
      "\tx=[0.81 0.95 0.   ... 0.83 0.   0.95]\n",
      "\ty=[0.]\n",
      "\ty=[0.99]\n",
      "\tExplanation: ~feature0000006749 & feature0000013710 & ~feature0000026329\n",
      "\n",
      "Input 5\n",
      "\tx=[0.   0.68 0.   ... 0.43 0.   0.89]\n",
      "\ty=[0.]\n",
      "\ty=[0.99]\n",
      "\tExplanation: ~feature0000006749 & ~feature0000010382 & ~feature0000013464 & ~feature0000013710 & ~feature0000022010 & ~feature0000025379 & ~feature0000026329\n",
      "\n",
      "Input 6\n",
      "\tx=[0.79 0.65 0.   ... 0.3  0.   0.69]\n",
      "\ty=[0.]\n",
      "\ty=[0.99]\n",
      "\tExplanation: \n",
      "\n",
      "Input 7\n",
      "\tx=[0.7  0.75 0.   ... 0.   0.52 0.42]\n",
      "\ty=[0.]\n",
      "\ty=[0.99]\n",
      "\tExplanation: ~feature0000006749 & ~feature0000010382 & feature0000013464 & ~feature0000013710 & ~feature0000022010 & ~feature0000025379 & ~feature0000026329\n",
      "\n",
      "Input 8\n",
      "\tx=[0.88 0.52 0.   ... 0.   0.33 0.74]\n",
      "\ty=[0.]\n",
      "\ty=[0.99]\n",
      "\tExplanation: ~feature0000013710\n",
      "\n",
      "Input 9\n",
      "\tx=[0.61 0.6  0.   ... 1.   0.   0.69]\n",
      "\ty=[0.]\n",
      "\ty=[0.99]\n",
      "\tExplanation: ~feature0000006749 & ~feature0000010382 & ~feature0000013464 & ~feature0000013710 & feature0000022010 & ~feature0000025379 & feature0000026329\n",
      "\n",
      "Input 10\n",
      "\tx=[0.76 0.07 0.   ... 0.41 0.88 0.1 ]\n",
      "\ty=[0.]\n",
      "\ty=[0.99]\n",
      "\tExplanation: ~feature0000006749 & feature0000013710\n",
      "\n",
      "Input 11\n",
      "\tx=[0.3  0.71 0.   ... 0.51 0.   0.97]\n",
      "\ty=[1.]\n",
      "\ty=[1.]\n",
      "\tExplanation: ~feature0000006749 & ~feature0000010382 & ~feature0000013464 & feature0000013710 & ~feature0000022010 & ~feature0000025379 & feature0000026329\n",
      "\n",
      "Input 12\n",
      "\tx=[0.6  0.51 0.   ... 0.23 0.   0.15]\n",
      "\ty=[1.]\n",
      "\ty=[1.]\n",
      "\tExplanation: ~feature0000006749 & feature0000010382 & feature0000013710 & ~feature0000022010 & feature0000025379\n",
      "\n",
      "Input 13\n",
      "\tx=[0.72 0.56 0.   ... 0.22 0.   0.82]\n",
      "\ty=[1.]\n",
      "\ty=[1.]\n",
      "\tExplanation: feature0000006749 & feature0000026329\n",
      "\n",
      "Input 14\n",
      "\tx=[0.31 0.53 0.   ... 0.12 0.   0.  ]\n",
      "\ty=[1.]\n",
      "\ty=[1.]\n",
      "\tExplanation: ~feature0000006749 & feature0000010382 & ~feature0000013464 & feature0000013710 & ~feature0000022010 & ~feature0000025379 & ~feature0000026329\n",
      "\n",
      "Input 15\n",
      "\tx=[0.96 0.57 0.   ... 0.44 0.54 0.59]\n",
      "\ty=[1.]\n",
      "\ty=[1.]\n",
      "\tExplanation: feature0000006749 & feature0000010382 & ~feature0000013464\n",
      "\n",
      "Input 16\n",
      "\tx=[0.86 0.53 0.   ... 0.69 0.08 0.73]\n",
      "\ty=[1.]\n",
      "\ty=[1.]\n",
      "\tExplanation: feature0000006749 & ~feature0000010382 & feature0000013464 & feature0000013710 & ~feature0000022010 & ~feature0000025379 & ~feature0000026329\n",
      "\n",
      "Input 17\n",
      "\tx=[0.83 0.67 0.   ... 0.   1.   0.5 ]\n",
      "\ty=[1.]\n",
      "\ty=[1.]\n",
      "\tExplanation: ~feature0000006749 & feature0000010382 & ~feature0000013464 & ~feature0000022010 & feature0000025379\n",
      "\n",
      "Input 18\n",
      "\tx=[0.95 0.67 0.   ... 0.68 0.   0.94]\n",
      "\ty=[1.]\n",
      "\ty=[1.]\n",
      "\tExplanation: feature0000006749 & ~feature0000010382 & ~feature0000013464 & ~feature0000013710 & ~feature0000022010 & feature0000025379 & feature0000026329\n",
      "\n",
      "Input 19\n",
      "\tx=[0.94 0.63 0.   ... 0.   0.   0.77]\n",
      "\ty=[1.]\n",
      "\ty=[1.]\n",
      "\tExplanation: feature0000006749 & feature0000010382 & ~feature0000013464 & feature0000013710 & ~feature0000022010 & ~feature0000025379 & ~feature0000026329\n",
      "\n",
      "Input 20\n",
      "\tx=[0.7  0.14 0.   ... 0.38 0.75 0.77]\n",
      "\ty=[1.]\n",
      "\ty=[1.]\n",
      "\tExplanation: feature0000006749 & ~feature0000010382 & ~feature0000013464 & feature0000013710 & feature0000022010 & ~feature0000025379 & ~feature0000026329\n",
      "\n",
      "Input 21\n",
      "\tx=[0.58 1.   0.   ... 0.   0.   0.56]\n",
      "\ty=[1.]\n",
      "\ty=[1.]\n",
      "\tExplanation: ~feature0000006749 & feature0000010382 & ~feature0000013464 & ~feature0000013710 & ~feature0000022010 & ~feature0000025379 & feature0000026329\n",
      "\n",
      "Input 22\n",
      "\tx=[0.7  0.63 0.   ... 0.86 0.   0.57]\n",
      "\ty=[1.]\n",
      "\ty=[1.]\n",
      "\tExplanation: ~feature0000006749 & feature0000010382 & ~feature0000013464 & feature0000013710 & ~feature0000022010 & ~feature0000025379 & ~feature0000026329\n",
      "\n",
      "Input 23\n",
      "\tx=[0.68 0.3  0.   ... 0.26 0.   0.87]\n",
      "\ty=[1.]\n",
      "\ty=[1.]\n",
      "\tExplanation: feature0000006749 & feature0000010382 & ~feature0000013464 & ~feature0000013710 & ~feature0000022010 & ~feature0000025379 & ~feature0000026329\n",
      "\n",
      "Input 24\n",
      "\tx=[0.8  0.41 0.   ... 0.54 0.   0.84]\n",
      "\ty=[1.]\n",
      "\ty=[1.]\n",
      "\tExplanation: feature0000013710 & ~feature0000022010 & feature0000025379\n",
      "\n",
      "Input 25\n",
      "\tx=[0.69 0.12 0.   ... 0.72 0.   0.34]\n",
      "\ty=[1.]\n",
      "\ty=[1.]\n",
      "\tExplanation: ~feature0000006749 & feature0000010382 & ~feature0000013464 & ~feature0000013710 & ~feature0000022010 & feature0000025379 & feature0000026329\n",
      "\n",
      "Input 26\n",
      "\tx=[0.61 0.82 0.   ... 0.85 0.   1.  ]\n",
      "\ty=[1.]\n",
      "\ty=[1.]\n",
      "\tExplanation: ~feature0000006749 & ~feature0000010382 & ~feature0000013464 & feature0000013710 & ~feature0000022010 & feature0000025379 & feature0000026329\n",
      "\n",
      "Input 27\n",
      "\tx=[1.   0.6  0.   ... 0.14 0.   0.64]\n",
      "\ty=[1.]\n",
      "\ty=[1.]\n",
      "\tExplanation: ~feature0000006749 & feature0000010382 & ~feature0000013464 & feature0000013710 & ~feature0000022010 & ~feature0000025379 & feature0000026329\n",
      "\n",
      "Input 28\n",
      "\tx=[0.37 0.75 0.   ... 0.07 0.   0.5 ]\n",
      "\ty=[1.]\n",
      "\ty=[1.]\n",
      "\tExplanation: feature0000013710 & ~feature0000022010 & feature0000025379\n",
      "\n",
      "Input 29\n",
      "\tx=[0.72 0.4  0.   ... 0.21 0.56 0.46]\n",
      "\ty=[1.]\n",
      "\ty=[1.]\n",
      "\tExplanation: feature0000006749 & ~feature0000010382 & ~feature0000013464 & ~feature0000013710 & ~feature0000022010 & ~feature0000025379 & feature0000026329\n",
      "\n",
      "Input 30\n",
      "\tx=[0.49 0.49 0.   ... 0.   0.   0.13]\n",
      "\ty=[1.]\n",
      "\ty=[1.]\n",
      "\tExplanation: ~feature0000006749 & ~feature0000010382 & ~feature0000013464 & feature0000013710 & ~feature0000022010 & feature0000025379 & feature0000026329\n",
      "\n",
      "Input 31\n",
      "\tx=[0.97 0.35 0.   ... 0.31 0.   0.41]\n",
      "\ty=[1.]\n",
      "\ty=[1.]\n",
      "\tExplanation: feature0000013710 & ~feature0000022010 & feature0000025379\n",
      "\n",
      "Input 32\n",
      "\tx=[0.86 0.57 0.   ... 0.38 0.01 0.92]\n",
      "\ty=[1.]\n",
      "\ty=[1.]\n",
      "\tExplanation: feature0000006749 & feature0000010382 & ~feature0000013464\n",
      "\n",
      "Input 33\n",
      "\tx=[0.56 0.72 0.   ... 0.29 1.   0.35]\n",
      "\ty=[1.]\n",
      "\ty=[1.]\n",
      "\tExplanation: ~feature0000006749 & feature0000010382 & ~feature0000013464 & feature0000013710 & ~feature0000022010 & ~feature0000025379 & feature0000026329\n",
      "\n",
      "Input 34\n",
      "\tx=[0.51 0.45 0.   ... 0.3  0.   0.3 ]\n",
      "\ty=[1.]\n",
      "\ty=[1.]\n",
      "\tExplanation: ~feature0000006749 & ~feature0000010382 & ~feature0000013464 & feature0000013710 & ~feature0000022010 & ~feature0000025379 & feature0000026329\n",
      "\n",
      "Input 35\n",
      "\tx=[0.3  0.28 0.   ... 0.   0.   0.42]\n",
      "\ty=[1.]\n",
      "\ty=[1.]\n",
      "\tExplanation: feature0000013710 & ~feature0000022010 & feature0000025379\n",
      "\n",
      "Input 36\n",
      "\tx=[0.71 0.78 0.   ... 0.   0.   0.54]\n",
      "\ty=[1.]\n",
      "\ty=[1.]\n",
      "\tExplanation: feature0000006749 & ~feature0000010382 & ~feature0000013464 & ~feature0000013710 & ~feature0000022010 & ~feature0000025379 & feature0000026329\n",
      "\n",
      "Input 37\n",
      "\tx=[0.79 0.97 0.   ... 0.57 0.23 0.62]\n",
      "\ty=[1.]\n",
      "\ty=[1.]\n",
      "\tExplanation: ~feature0000006749 & feature0000010382 & ~feature0000013464 & feature0000013710 & ~feature0000022010 & feature0000025379 & ~feature0000026329\n",
      "\n",
      "Input 38\n",
      "\tx=[0.74 0.25 0.   ... 0.   0.83 0.48]\n",
      "\ty=[1.]\n",
      "\ty=[1.]\n",
      "\tExplanation: ~feature0000006749 & feature0000010382 & ~feature0000013464 & feature0000013710 & ~feature0000022010 & ~feature0000025379 & feature0000026329\n",
      "\n",
      "Input 39\n",
      "\tx=[0.57 0.   0.   ... 0.29 0.39 0.26]\n",
      "\ty=[1.]\n",
      "\ty=[1.]\n",
      "\tExplanation: ~feature0000006749 & feature0000010382 & ~feature0000013464 & feature0000013710 & ~feature0000022010 & ~feature0000025379 & feature0000026329\n",
      "\n",
      "Input 40\n",
      "\tx=[0.56 0.24 1.   ... 0.   0.08 0.68]\n",
      "\ty=[1.]\n",
      "\ty=[1.]\n",
      "\tExplanation: ~feature0000006749 & feature0000010382 & ~feature0000013464 & ~feature0000013710 & ~feature0000022010 & ~feature0000025379 & feature0000026329\n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(precision=2, suppress=True)\n",
    "outputs = []\n",
    "for i, (xin, yin) in enumerate(zip(x, y)):\n",
    "    model_reduced = get_reduced_model(model, xin.to(device)).to(device)\n",
    "    for module in model_reduced.children():\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            wa = module.weight.cpu().detach().numpy()\n",
    "            break\n",
    "    output = model_reduced(xin)\n",
    "    \n",
    "    pred_class = torch.argmax(output)\n",
    "    true_class = torch.argmax(y[i])\n",
    "\n",
    "    # generate local explanation only if the prediction is correct\n",
    "    if pred_class.eq(true_class):\n",
    "        local_explanation = fol.relunn.explain_local(model.to(device), x, y, xin, yin, device=device)\n",
    "        print(f'Input {(i+1)}')\n",
    "        print(f'\\tx={xin.cpu().detach().numpy()}')\n",
    "        print(f'\\ty={y[i].cpu().detach().numpy()}')\n",
    "        print(f'\\ty={output.cpu().detach().numpy()}')\n",
    "        #print(f'\\tw={wa}')\n",
    "        print(f'\\tExplanation: {local_explanation}')\n",
    "        print()\n",
    "    outputs.append(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine local explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dummy_names = [f'g{i}' for i in range(x.shape[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ILMN_1343291', 'ILMN_1343295', 'ILMN_1651199', ...,\n",
       "       'ILMN_3311165', 'ILMN_3311180', 'ILMN_3311190'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concept_names = genes.values.squeeze()\n",
    "concept_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "y1h = OneHotEncoder(sparse=False).fit_transform(y.cpu().detach().numpy())\n",
    "y2 = torch.FloatTensor(y1h).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of when using the formula \"(ILMN_1777811 & ILMN_2241168 & ~ILMN_1708983 & ~ILMN_1745049 & ~ILMN_3228700 & ~ILMN_3243714) | (ILMN_3243714 & ~ILMN_1708983 & ~ILMN_1745049 & ~ILMN_1777811 & ~ILMN_2241168 & ~ILMN_3228700) | (~ILMN_1708983 & ~ILMN_1775520 & ~ILMN_1777811 & ~ILMN_2241168 & ~ILMN_3228700 & ~ILMN_3243714)\": 0.9250\n",
      "\n",
      "Accuracy of when using the formula \"(g13710 & g22010 & ~g6749 & ~g10382 & ~g25379 & ~g26329) | (g26329 & ~g6749 & ~g10382 & ~g13710 & ~g22010 & ~g25379) | (~g6749 & ~g13464 & ~g13710 & ~g22010 & ~g25379 & ~g26329)\": 0.9250\n"
     ]
    }
   ],
   "source": [
    "global_explanation, predictions, counter = fol.combine_local_explanations(model, \n",
    "                                                                          x=x, y=y2, is_pruned=True, \n",
    "                                                                          target_class=0,\n",
    "                                                                          topk_explanations=8,\n",
    "                                                                          device=device)\n",
    "\n",
    "accuracy, preds = fol.base.test_explanation(global_explanation, 0, x, y)\n",
    "final_formula = fol.base.replace_names(global_explanation, concept_names)\n",
    "final_formula_with_dummies = fol.base.replace_names(global_explanation, dummy_names)\n",
    "print(f'Accuracy of when using the formula \"{final_formula}\": {accuracy:.4f}\\n')\n",
    "print(f'Accuracy of when using the formula \"{final_formula_with_dummies}\": {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of when using the formula \"(ILMN_1745049 & ILMN_1777811 & ~ILMN_1708983 & ~ILMN_1775520 & ~ILMN_2241168) | (ILMN_1745049 & ILMN_3243714 & ~ILMN_1708983 & ~ILMN_1775520 & ~ILMN_2241168) | (ILMN_1777811 & ILMN_3243714 & ~ILMN_1708983 & ~ILMN_1775520 & ~ILMN_2241168) | (ILMN_1745049 & ILMN_1777811 & ILMN_3228700 & ILMN_3243714 & ~ILMN_1775520 & ~ILMN_2241168) | (ILMN_1708983 & ILMN_1745049 & ILMN_2241168 & ILMN_3228700 & ILMN_3243714 & ~ILMN_1775520 & ~ILMN_1777811) | (ILMN_1708983 & ILMN_3243714 & ~ILMN_1745049 & ~ILMN_1775520 & ~ILMN_2241168 & ~ILMN_3228700) | (ILMN_1708983 & ILMN_1775520 & ILMN_1777811 & ~ILMN_1745049 & ~ILMN_2241168 & ~ILMN_3228700 & ~ILMN_3243714) | (ILMN_1708983 & ILMN_1777811 & ILMN_2241168 & ~ILMN_1745049 & ~ILMN_1775520 & ~ILMN_3228700 & ~ILMN_3243714) | (ILMN_1708983 & ILMN_1745049 & ~ILMN_1775520 & ~ILMN_1777811 & ~ILMN_2241168 & ~ILMN_3228700 & ~ILMN_3243714)\": 0.9250\n",
      "\n",
      "Accuracy of when using the formula \"(g10382 & g13710 & ~g6749 & ~g13464 & ~g22010) | (g10382 & g26329 & ~g6749 & ~g13464 & ~g22010) | (g13710 & g26329 & ~g6749 & ~g13464 & ~g22010) | (g10382 & g13710 & g25379 & g26329 & ~g13464 & ~g22010) | (g6749 & g10382 & g22010 & g25379 & g26329 & ~g13464 & ~g13710) | (g6749 & g26329 & ~g10382 & ~g13464 & ~g22010 & ~g25379) | (g6749 & g13464 & g13710 & ~g10382 & ~g22010 & ~g25379 & ~g26329) | (g6749 & g13710 & g22010 & ~g10382 & ~g13464 & ~g25379 & ~g26329) | (g6749 & g10382 & ~g13464 & ~g13710 & ~g22010 & ~g25379 & ~g26329)\": 0.9250\n"
     ]
    }
   ],
   "source": [
    "global_explanation, predictions, counter = fol.combine_local_explanations(model, \n",
    "                                                                          x=x, y=y2, is_pruned=True, \n",
    "                                                                          target_class=1,\n",
    "                                                                          topk_explanations=15,\n",
    "                                                                          device=device)\n",
    "\n",
    "accuracy, preds = fol.base.test_explanation(global_explanation, 1, x, y)\n",
    "final_formula = fol.base.replace_names(global_explanation, concept_names)\n",
    "final_formula_with_dummies = fol.base.replace_names(global_explanation, dummy_names)\n",
    "print(f'Accuracy of when using the formula \"{final_formula}\": {accuracy:.4f}\\n')\n",
    "print(f'Accuracy of when using the formula \"{final_formula_with_dummies}\": {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6749</th>\n",
       "      <td>ILMN_1708983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10382</th>\n",
       "      <td>ILMN_1745049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13464</th>\n",
       "      <td>ILMN_1775520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13710</th>\n",
       "      <td>ILMN_1777811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22010</th>\n",
       "      <td>ILMN_2241168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25379</th>\n",
       "      <td>ILMN_3228700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26329</th>\n",
       "      <td>ILMN_3243714</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0\n",
       "6749   ILMN_1708983\n",
       "10382  ILMN_1745049\n",
       "13464  ILMN_1775520\n",
       "13710  ILMN_1777811\n",
       "22010  ILMN_2241168\n",
       "25379  ILMN_3228700\n",
       "26329  ILMN_3243714"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w, b = collect_parameters(model, device)\n",
    "feature_weights = w[0]\n",
    "feature_used_bool = np.sum(np.abs(feature_weights), axis=0) > 0\n",
    "feature_used = np.nonzero(feature_used_bool)[0]\n",
    "genes.iloc[feature_used]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0 - Global explanation: \"(feature0000006749 & feature0000010382 & feature0000013710) | (feature0000006749 & feature0000010382 & feature0000025379) | (feature0000006749 & feature0000010382 & feature0000026329) | (feature0000006749 & feature0000013710 & feature0000025379) | (feature0000006749 & feature0000013710 & feature0000026329) | (feature0000006749 & feature0000025379 & feature0000026329) | (feature0000010382 & feature0000013710 & feature0000025379) | (feature0000010382 & feature0000013710 & feature0000026329) | (feature0000010382 & feature0000025379 & feature0000026329) | (feature0000013710 & feature0000025379 & feature0000026329) | (feature0000006749 & feature0000010382 & ~feature0000013464) | (feature0000006749 & feature0000010382 & ~feature0000022010) | (feature0000006749 & feature0000013710 & ~feature0000013464) | (feature0000006749 & feature0000013710 & ~feature0000022010) | (feature0000006749 & feature0000025379 & ~feature0000022010) | (feature0000006749 & feature0000026329 & ~feature0000022010) | (feature0000010382 & feature0000013710 & ~feature0000013464) | (feature0000010382 & feature0000013710 & ~feature0000022010) | (feature0000010382 & feature0000025379 & ~feature0000022010) | (feature0000010382 & feature0000026329 & ~feature0000022010) | (feature0000013710 & feature0000025379 & ~feature0000022010) | (feature0000013710 & feature0000026329 & ~feature0000022010) | (feature0000025379 & feature0000026329 & ~feature0000022010) | (feature0000006749 & ~feature0000013464 & ~feature0000022010) | (feature0000010382 & ~feature0000013464 & ~feature0000022010) | (feature0000013710 & ~feature0000013464 & ~feature0000022010)\" - Accuracy: 0.0250\n"
     ]
    }
   ],
   "source": [
    "global_explanation2 = fol.relunn.explain_global(model, n_classes=1, target_class=0, device=device)\n",
    "explanation = fol.relunn.explain_global(model, n_classes=1, target_class=0, device=device)\n",
    "if explanation not in ['False', 'True', 'The formula is too complex!']:\n",
    "    accuracy, _ = fol.relunn.test_explanation(explanation, target_class=0, x=x.cpu(), y=y.cpu())\n",
    "    print(f'Class {0} - Global explanation: \"{global_explanation2}\" - Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From previous works\n",
    "\n",
    "ILMN_3286286, **ILMN_1775520**, ILMN_1656849, ILMN_1781198, ILMN_1665457"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn(x_train, y_train, layers, device):\n",
    "    torch.manual_seed(0)\n",
    "    np.random.seed(0)\n",
    "    model = torch.nn.Sequential(*layers).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "    model.train()\n",
    "    need_pruning = True\n",
    "    for epoch in range(15000):\n",
    "        # forward pass\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x_train)\n",
    "        # Compute Loss\n",
    "        loss = torch.nn.functional.mse_loss(y_pred, y_train)\n",
    "\n",
    "        for module in model.children():\n",
    "            if isinstance(module, torch.nn.Linear):\n",
    "                loss += 0.01 * torch.norm(module.weight, 1)\n",
    "                loss += 0.01 * torch.norm(module.bias, 1)\n",
    "\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch > 8000 and need_pruning and epoch % 1000 == 0:\n",
    "            prune_features(model, 1, device)\n",
    "            need_pruning = True\n",
    "    \n",
    "    global_explanation, predictions, counter = fol.combine_local_explanations(model, \n",
    "                                                                          x=x_train, y=y_train, is_pruned=True, \n",
    "                                                                          target_class=0,\n",
    "                                                                          topk_explanations=20,\n",
    "                                                                          device=device)\n",
    "\n",
    "    return model, global_explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split [1/5]\n",
      "\tAccuracy: 0.6250\n",
      "Split [2/5]\n",
      "\tAccuracy: 0.5000\n",
      "Split [3/5]\n",
      "\tAccuracy: 0.7500\n",
      "Split [4/5]\n",
      "\tAccuracy: 0.8750\n",
      "Split [5/5]\n",
      "\tAccuracy: 0.5000\n"
     ]
    }
   ],
   "source": [
    "n_splits = 5\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=0)\n",
    "classifier = DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(skf.split(x.cpu().detach().numpy(), y.cpu().detach().numpy())):\n",
    "    print(f'Split [{i+1}/{n_splits}]')\n",
    "    x_train, x_test = x[train_index].cpu().detach().numpy(), x[test_index].cpu().detach().numpy()\n",
    "    y_train, y_test = y[train_index].cpu().detach().numpy().squeeze(), y[test_index].cpu().detach().numpy().squeeze()\n",
    "    \n",
    "    classifier.fit(x_train, y_train)\n",
    "    y_preds = classifier.predict(x_test)\n",
    "    accuracy = accuracy_score(y_test, y_preds)\n",
    "    print(f'\\tAccuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_tree(classifier, feature_names=concept_names, filled=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split [1/5]\n",
      "\tAccuracy: 1.0000\n",
      "Split [2/5]\n",
      "\tAccuracy: 1.0000\n",
      "Split [3/5]\n",
      "\tAccuracy: 1.0000\n",
      "Split [4/5]\n",
      "\tAccuracy: 1.0000\n",
      "Split [5/5]\n",
      "\tAccuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "n_splits = 5\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=0)\n",
    "classifier = DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "explanations = []\n",
    "for i, (train_index, test_index) in enumerate(skf.split(x.cpu().detach().numpy(), y.cpu().detach().numpy())):\n",
    "    print(f'Split [{i+1}/{n_splits}]')\n",
    "    x_train, x_test = x[train_index], x[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    model, explanation = train_nn(x_train, y_train, layers, device)\n",
    "    explanations.append(explanation)\n",
    "    \n",
    "    y_preds = (model(x_test) > 0.5).cpu().detach().numpy()\n",
    "    accuracy = accuracy_score(y_test.cpu().detach().numpy(), y_preds)\n",
    "    print(f'\\tAccuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split [1/5] \n",
      "\t Formula: \"(~ILMN_1708983 & ~ILMN_1745049 & ~ILMN_1777811) | (~ILMN_1708983 & ~ILMN_1745049 & ~ILMN_3243714) | (~ILMN_1708983 & ~ILMN_1777811 & ~ILMN_3243714)\" \n",
      "\t Test accuracy (formula): 100.00%\n",
      "\n",
      "Split [2/5] \n",
      "\t Formula: \"(~ILMN_1708983 & ~ILMN_1745049) | (~ILMN_1708983 & ~ILMN_3243714)\" \n",
      "\t Test accuracy (formula): 87.50%\n",
      "\n",
      "Split [3/5] \n",
      "\t Formula: \"(~ILMN_1708983 & ~ILMN_1745049) | (~ILMN_1708983 & ~ILMN_3243714)\" \n",
      "\t Test accuracy (formula): 87.50%\n",
      "\n",
      "Split [4/5] \n",
      "\t Formula: \"~ILMN_1708983 & ~ILMN_1745049\" \n",
      "\t Test accuracy (formula): 100.00%\n",
      "\n",
      "Split [5/5] \n",
      "\t Formula: \"(~ILMN_1708983 & ~ILMN_1745049) | (~ILMN_1708983 & ~ILMN_3243714)\" \n",
      "\t Test accuracy (formula): 87.50%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, explanation in enumerate(explanations):\n",
    "    accuracy, preds = fol.base.test_explanation(explanation, 0, x_test, y_test)\n",
    "    final_formula = fol.base.replace_names(explanation, concept_names)\n",
    "    print(f'Split [{i+1}/{n_splits}] \\n\\t Formula: \"{final_formula}\" \\n\\t Test accuracy (formula): {accuracy*100:.2f}%\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
