{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import sklearn\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "sys.path.append('..')\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from deep_logic.utils.relunn import get_reduced_model, prune_features\n",
    "from deep_logic import fol\n",
    "import deep_logic as dl\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (1797, 64)\n",
      "Classes: [0. 1.]\n",
      "X max: 1.0\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "X, y = load_digits(return_X_y=True)\n",
    "y = sklearn.preprocessing.OneHotEncoder(sparse=False).fit_transform(y.reshape(-1, 1))\n",
    "X -= X.min()\n",
    "X /= X.max()\n",
    "print(f'X shape: {X.shape}\\nClasses: {np.unique(y)}')\n",
    "print(f'X max: {X.max()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (1617, 64)\n",
      "Y shape: (1617, 10)\n",
      "X_test shape: (180, 64)\n",
      "Y_test shape: (180, 10)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X, X_test, y, y_test = train_test_split(X, y, test_size=0.1)\n",
    "print(f'X shape: {X.shape}\\nY shape: {y.shape}')\n",
    "print(f'X_test shape: {X_test.shape}\\nY_test shape: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['f_0',\n",
       " 'f_1',\n",
       " 'f_2',\n",
       " 'f_3',\n",
       " 'f_4',\n",
       " 'f_5',\n",
       " 'f_6',\n",
       " 'f_7',\n",
       " 'f_8',\n",
       " 'f_9',\n",
       " 'f_10',\n",
       " 'f_11',\n",
       " 'f_12',\n",
       " 'f_13',\n",
       " 'f_14',\n",
       " 'f_15',\n",
       " 'f_16',\n",
       " 'f_17',\n",
       " 'f_18',\n",
       " 'f_19',\n",
       " 'f_20',\n",
       " 'f_21',\n",
       " 'f_22',\n",
       " 'f_23',\n",
       " 'f_24',\n",
       " 'f_25',\n",
       " 'f_26',\n",
       " 'f_27',\n",
       " 'f_28',\n",
       " 'f_29',\n",
       " 'f_30',\n",
       " 'f_31',\n",
       " 'f_32',\n",
       " 'f_33',\n",
       " 'f_34',\n",
       " 'f_35',\n",
       " 'f_36',\n",
       " 'f_37',\n",
       " 'f_38',\n",
       " 'f_39',\n",
       " 'f_40',\n",
       " 'f_41',\n",
       " 'f_42',\n",
       " 'f_43',\n",
       " 'f_44',\n",
       " 'f_45',\n",
       " 'f_46',\n",
       " 'f_47',\n",
       " 'f_48',\n",
       " 'f_49',\n",
       " 'f_50',\n",
       " 'f_51',\n",
       " 'f_52',\n",
       " 'f_53',\n",
       " 'f_54',\n",
       " 'f_55',\n",
       " 'f_56',\n",
       " 'f_57',\n",
       " 'f_58',\n",
       " 'f_59',\n",
       " 'f_60',\n",
       " 'f_61',\n",
       " 'f_62',\n",
       " 'f_63']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concept_names = [f\"f_{i}\" for i in range(X.shape[1])]\n",
    "concept_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "torch.Size([1617, 64])\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "print(y)\n",
    "x_train = torch.tensor(X, dtype=torch.float)\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 1., 0.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "y_train = torch.zeros((y.shape[0], y.shape[1]), dtype=torch.float)\n",
    "y_train = torch.tensor(y, dtype=torch.float)\n",
    "x_test = x_train\n",
    "n_classes = y_train.size(1)\n",
    "print(n_classes)\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([167., 162., 161., 173., 171., 161., 156., 159., 151., 156.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "y_train.sum(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\Anaconda3\\envs\\fsc\\lib\\site-packages\\torch\\nn\\modules\\container.py:117: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100: train accuracy: 0.9351\n",
      "Epoch 200: train accuracy: 0.9889\n",
      "Epoch 300: train accuracy: 0.9969\n",
      "Epoch 400: train accuracy: 0.9981\n",
      "Epoch 500: train accuracy: 0.9994\n",
      "Epoch 600: train accuracy: 0.9994\n",
      "Epoch 700: train accuracy: 1.0000\n",
      "Epoch 800: train accuracy: 1.0000\n",
      "Epoch 900: train accuracy: 1.0000\n",
      "Epoch 1000: train accuracy: 0.9963\n",
      "Epoch 1100: train accuracy: 0.9988\n",
      "Epoch 1200: train accuracy: 0.9988\n",
      "Epoch 1300: train accuracy: 1.0000\n",
      "Epoch 1400: train accuracy: 1.0000\n",
      "Epoch 1500: train accuracy: 1.0000\n",
      "Epoch 1600: train accuracy: 0.9388\n",
      "Epoch 1700: train accuracy: 0.9481\n",
      "Epoch 1800: train accuracy: 0.9511\n",
      "Epoch 1900: train accuracy: 0.9549\n",
      "Epoch 2000: train accuracy: 0.9579\n",
      "Epoch 2100: train accuracy: 0.9592\n",
      "Epoch 2200: train accuracy: 0.9579\n",
      "Epoch 2300: train accuracy: 0.9586\n",
      "Epoch 2400: train accuracy: 0.9604\n",
      "Epoch 2500: train accuracy: 0.9623\n",
      "Epoch 2600: train accuracy: 0.9641\n",
      "Epoch 2700: train accuracy: 0.9666\n",
      "Epoch 2800: train accuracy: 0.9678\n",
      "Epoch 2900: train accuracy: 0.9691\n",
      "Epoch 3000: train accuracy: 0.9709\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "layers = [\n",
    "    torch.nn.Linear(x_train.size(1), 50 * n_classes),\n",
    "    torch.nn.LeakyReLU(),\n",
    "    dl.nn.XLinear(50, 20, n_classes),\n",
    "    torch.nn.LeakyReLU(),\n",
    "    dl.nn.XLinear(20, 10, n_classes),\n",
    "    torch.nn.LeakyReLU(),\n",
    "    dl.nn.XLinear(10, 1, n_classes),\n",
    "    torch.nn.Softmax(),\n",
    "]\n",
    "model = torch.nn.Sequential(*layers)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_form = torch.nn.BCELoss()\n",
    "model.train()\n",
    "need_pruning = True\n",
    "for epoch in range(3000):\n",
    "    # forward pass\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model(x_train)\n",
    "    # Compute Loss\n",
    "    loss = loss_form(y_pred, y_train)\n",
    "\n",
    "    for module in model.children():\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            loss += 0.0001 * torch.norm(module.weight, 1)\n",
    "            break\n",
    "\n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch > 1500 and need_pruning:\n",
    "        prune_features(model, n_classes)\n",
    "        need_pruning = False\n",
    "\n",
    "    # compute accuracy\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        y_pred_d = torch.argmax(y_pred, dim=1)\n",
    "        y_train_d = torch.argmax(y_train, dim=1)\n",
    "        accuracy = y_pred_d.eq(y_train_d).sum().item() / y_train.size(0)\n",
    "        print(f'Epoch {epoch + 1}: train accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input 1\n",
      "\tx=[0.   0.   0.   0.56 0.69 0.   0.   0.   0.   0.   0.12 0.94 0.5  0.\n",
      " 0.   0.   0.   0.   0.69 0.94 0.06 0.19 0.5  0.   0.   0.38 1.   0.25\n",
      " 0.   0.88 0.75 0.   0.   0.75 1.   0.25 0.69 1.   0.31 0.   0.   0.56\n",
      " 1.   1.   1.   0.69 0.   0.   0.   0.   0.38 0.69 1.   0.44 0.   0.\n",
      " 0.   0.   0.   0.62 1.   0.25 0.   0.  ]\n",
      "\ty=[0.   0.91 0.   0.   1.   0.02 0.99 0.96 0.   0.  ], y_label=tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "\tw=[[  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.   -29.57   0.\n",
      "    0.     0.     0.     0.     0.     0.   -26.16   0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     2.6    0.     0.     0.     0.     0.     9.99\n",
      "    0.     0.     0.     0.     0.     0.     0.     3.99   0.     0.\n",
      "    0.     0.     0.     0.     0.    -1.93   0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.   -37.36   0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     4.63\n",
      "    0.     0.     0.    11.48]\n",
      " [  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.    -4.55  -4.06   0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.   -13.76   3.51   0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.    37.9    0.  ]\n",
      " [  0.     0.     0.     0.     2.15   0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.38\n",
      "    0.     0.39   0.     0.     0.     0.    -3.58   0.     0.    -3.36\n",
      "   -4.35   0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.    -3.7    0.     0.     1.81   0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.    -8.38   0.     0.   -11.6    0.     0.     0.\n",
      "    0.     0.    -5.51   0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "   11.53   0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     7.48   0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.   -21.23   0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.  ]\n",
      " [  0.     0.     5.66   0.     0.     1.86   0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "   -3.47  -4.2   -1.1    0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    7.32   0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "  -12.99   5.73   0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     5.53   0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.    21.28   0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     2.67   0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.    -2.13   0.     0.\n",
      "    7.64   0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.    -5.43   0.     0.     0.     0.     0.     0.\n",
      "    0.    -5.88   0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     2.89   0.     0.     0.     0.     0.    -1.53   0.     0.\n",
      "    0.     0.     0.     0.    -4.04   0.     0.     0.     0.     0.\n",
      "    0.     0.    -3.85  -5.75   0.     0.     0.     0.     0.     0.\n",
      "    8.84   0.     0.     0.     0.     0.     0.     0.    -1.08   0.\n",
      "   -0.39   0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     1.63   0.     0.     0.     0.     0.     1.3    0.     0.\n",
      "    0.     0.     0.    -1.07   0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.    -1.34  -0.8    0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.  ]]\n",
      "\tb=[ 10.48 -10.51   8.    -7.17   7.45  -2.29  -6.85   0.59   3.58  -5.63]\n",
      "\tExplanation: feature0000000003 & ~feature0000000006 & ~feature0000000012 & feature0000000030 & feature0000000041 & ~feature0000000054\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input 3\n",
      "\tx=[0.   0.   0.   0.62 0.81 0.06 0.   0.   0.   0.06 0.69 0.75 0.44 0.\n",
      " 0.   0.   0.   0.12 1.   0.75 0.   0.   0.   0.   0.   0.25 1.   0.69\n",
      " 0.   0.   0.   0.   0.   0.25 1.   0.94 0.5  0.25 0.   0.   0.   0.25\n",
      " 1.   1.   0.81 1.   0.38 0.   0.   0.   0.44 1.   0.44 0.81 0.88 0.\n",
      " 0.   0.   0.   0.44 0.94 0.94 0.31 0.  ]\n",
      "\ty=[0.07 0.   0.01 0.   0.   0.5  1.   0.   0.   0.  ], y_label=tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "\tw=[[  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.   -29.57   0.\n",
      "    0.     0.     0.     0.     0.     0.   -26.16   0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     2.4    0.     0.     0.     0.     0.     9.22\n",
      "    0.     0.     0.     0.     0.     0.     0.     3.67   0.     0.\n",
      "    0.     0.     0.     0.     0.    -1.77   0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.   -34.26   0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     4.26\n",
      "    0.     0.     0.    10.54]\n",
      " [  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.    -3.15  -6.55   0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.    -7.45  -6.71   0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     6.12   0.  ]\n",
      " [  0.     0.     0.     0.     1.64   0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     4.92\n",
      "    0.     3.62   0.     0.     0.     0.    -4.89   0.     0.    -2.63\n",
      "   -0.01   0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.    -7.46   0.     0.    -0.44   0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.    -2.6    0.     0.    -3.52   0.     0.     0.\n",
      "    0.     0.    -1.75   0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    3.51   0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     2.24   0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.    -6.19   0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.  ]\n",
      " [  0.     0.    18.36   0.     0.    10.67   0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "  -14.97 -16.05  -9.23   0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "   -0.56   0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "   -1.21  -1.65   0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     1.39   0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.23   0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     1.07   0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.    -4.2    0.     0.\n",
      "    8.7    0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.    -5.49   0.     0.     0.     0.     0.     0.\n",
      "    0.    -5.42   0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     2.89   0.     0.     0.     0.     0.    -1.53   0.     0.\n",
      "    0.     0.     0.     0.    -4.04   0.     0.     0.     0.     0.\n",
      "    0.     0.    -3.85  -5.75   0.     0.     0.     0.     0.     0.\n",
      "    8.85   0.     0.     0.     0.     0.     0.     0.    -1.08   0.\n",
      "   -0.39   0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     1.98   0.     0.     0.     0.     0.     0.78   0.     0.\n",
      "    0.     0.     0.    -1.17   0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.    -1.16  -0.71   0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.  ]]\n",
      "\tb=[ 10.49 -10.73  13.85  -3.17  -2.47  -0.67  12.69  -6.24   3.59  -5.62]\n",
      "\tExplanation: feature0000000010 & ~feature0000000020 & ~feature0000000021 & feature0000000042 & feature0000000061\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input 4\n",
      "\tx=[0.   0.   0.31 0.94 1.   0.94 0.06 0.   0.   0.62 1.   0.69 0.5  1.\n",
      " 0.31 0.   0.   0.75 0.62 0.06 0.62 0.94 0.06 0.   0.   0.   0.   0.5\n",
      " 1.   0.69 0.06 0.   0.   0.   0.   0.06 0.62 1.   0.62 0.   0.   0.\n",
      " 0.   0.12 0.   0.44 1.   0.   0.   0.   0.5  0.81 0.31 0.94 0.75 0.\n",
      " 0.   0.   0.31 0.94 1.   0.88 0.19 0.  ]\n",
      "\ty=[0.   0.   0.97 1.   0.   0.01 0.   0.   0.06 1.  ], y_label=tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "\tw=[[  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.    -5.27   0.\n",
      "    0.     0.     0.     0.     0.     0.    -4.66   0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     7.2    0.     0.     0.     0.     0.     5.94\n",
      "    0.     0.     0.     0.     0.     0.     0.     2.56   0.     0.\n",
      "    0.     0.     0.     0.     0.    -2.28   0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.    -0.03   0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.   -18.09\n",
      "    0.     0.     0.     5.81]\n",
      " [  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.    -9.73   3.65   0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.    18.87 -17.08   0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.    -3.15   0.  ]\n",
      " [  0.     0.     0.     0.     4.68   0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.    11.29\n",
      "    0.     8.53   0.     0.     0.     0.   -12.35   0.     0.    -6.99\n",
      "   -0.03   0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.   -18.89   0.     0.    -1.37   0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.    -2.6    0.     0.    -3.52   0.     0.     0.\n",
      "    0.     0.    -1.75   0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    3.51   0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     2.24   0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.    -6.19   0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.  ]\n",
      " [  0.     0.     6.1    0.     0.     2.92   0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "   -3.43  -4.29  -2.58   0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "   -4.7    0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "   -9.75 -13.54   0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.    11.16   0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     1.6    0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     1.19   0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.    -4.61   0.     0.\n",
      "    9.57   0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.    -6.04   0.     0.     0.     0.     0.     0.\n",
      "    0.    -5.96   0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.56   0.     0.     0.     0.     0.    14.58   0.     0.\n",
      "    0.     0.     0.     0.    10.63   0.     0.     0.     0.     0.\n",
      "    0.     0.    17.97  12.12   0.     0.     0.     0.     0.     0.\n",
      "  -10.27   0.     0.     0.     0.     0.     0.     0.    -4.16   0.\n",
      "   -0.29   0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.    10.16   0.     0.     0.     0.     0.     7.58   0.     0.\n",
      "    0.     0.     0.    -5.86   0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.   -10.84  -4.17   0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.  ]]\n",
      "\tb=[-10.04  -9.62   9.68   6.    -2.47  -2.84  -9.77  -5.06  -5.35  -5.53]\n",
      "\tExplanation: feature0000000004 & ~feature0000000019 & feature0000000021 & ~feature0000000026 & feature0000000029 & ~feature0000000030 & ~feature0000000043 & feature0000000046\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input 5\n",
      "\tx=[0.   0.   0.56 0.81 1.   0.31 0.   0.   0.   0.19 1.   0.5  0.25 0.81\n",
      " 0.   0.   0.   0.38 0.62 0.06 0.   0.56 0.12 0.   0.   0.31 0.25 0.\n",
      " 0.   0.25 0.5  0.   0.   0.5  0.25 0.   0.   0.25 0.5  0.   0.   0.38\n",
      " 0.38 0.   0.   0.25 0.56 0.   0.   0.   0.81 0.12 0.   0.44 0.5  0.\n",
      " 0.   0.   0.5  0.75 0.81 0.94 0.12 0.  ]\n",
      "\ty=[1.   0.   0.06 1.   0.06 0.89 0.   0.   0.   0.02], y_label=tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "\tw=[[  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.   -29.57   0.\n",
      "    0.     0.     0.     0.     0.     0.   -26.16   0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     6.41   0.     0.     0.     0.     0.     5.31\n",
      "    0.     0.     0.     0.     0.     0.     0.     2.16   0.     0.\n",
      "    0.     0.     0.     0.     0.    -1.96   0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.    -0.55   0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.   -16.31\n",
      "    0.     0.     0.     5.36]\n",
      " [  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.   -13.03  10.24   0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.    27.38 -11.24   0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.    20.13   0.  ]\n",
      " [  0.     0.     0.     0.    10.56   0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     2.33\n",
      "    0.     2.12   0.     0.     0.     0.   -17.62   0.     0.   -16.34\n",
      "  -22.13   0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.   -17.77   0.     0.     9.63   0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.    -8.38   0.     0.   -11.6    0.     0.     0.\n",
      "    0.     0.    -5.51   0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "   11.53   0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     7.48   0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.   -21.24   0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.  ]\n",
      " [  0.     0.     7.44   0.     0.    11.95   0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "  -14.64 -10.8   -9.27   0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "   -7.82   0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "  -16.59 -22.84   0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.    19.03   0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     2.94   0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     3.89   0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.    -4.89   0.     0.\n",
      "   13.93   0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.    -9.89   0.     0.     0.     0.     0.     0.\n",
      "    0.   -10.52   0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     4.52   0.     0.     0.     0.     0.     5.81   0.     0.\n",
      "    0.     0.     0.     0.     3.37   0.     0.     0.     0.     0.\n",
      "    0.     0.     4.36   4.58   0.     0.     0.     0.     0.     0.\n",
      "   -5.77   0.     0.     0.     0.     0.     0.     0.    -0.22   0.\n",
      "   -2.6    0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     3.6    0.     0.     0.     0.     0.     2.74   0.     0.\n",
      "    0.     0.     0.    -2.17   0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.    -3.93  -1.55   0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.  ]]\n",
      "\tb=[10.49 -9.88  0.85  8.19  7.45  1.44 -0.09 -1.25 -4.94 -4.95]\n",
      "\tExplanation: ~feature0000000028 & ~feature0000000036\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAL4AAADCCAYAAAD3lHgnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAJU0lEQVR4nO3dX4hd1RXH8e+vo9JalYEmLeLEjhURpNBYQqAEitW2xCo6D31IQMmUgk8WQwuifdG+iyQPpSDRRtAqrX9AxGoFDVZorUmcWGO0pGFKprFNQgnGVirR1Yd7A2NmJtljzz7nHtfvA0Pm3jnsLMJvDif7nHWXIgKzbD7TdQFmXXDwLSUH31Jy8C0lB99ScvAtpbNqLLpixYqYnJyssXRjDh482Oh6x48fb3Q9gPfff7/R9S699NJG1wMYHx9vfM0mzc7OcvToUZ36fpXgT05OsnPnzhpLN2bz5s2Nrrdjx45G1wPYs2dPo+vdc889ja4HMDU11fiaTVqzZs2i7/tSx1Jy8C0lB99ScvAtpaLgS1ov6W1J+yXdUbsos9rOGHxJY8DPgWuBK4CNkq6oXZhZTSVn/LXA/og4EBEfAI8CN9Yty6yukuBfBMy/2zM3fM+st0qCv+CuF7Cge0XSLZJ2Stp55MiR/78ys4pKgj8HrJr3egI4dOpBEXFfRKyJiDUrV65sqj6zKkqC/ypwmaRLJJ0DbACeqluWWV1nfFYnIk5IuhV4DhgDHoiIvdUrM6uo6CG1iHgGeKZyLWat8Z1bS8nBt5QcfEvJwbeUqnRgNW1mZqbxNbdu3droenfddVej6wFs2bKl0fWa7jqD0e/AWorP+JaSg28pOfiWkoNvKTn4lpKDbyk5+JZSSc/tA5IOS3qjjYLM2lByxt8OrK9ch1mrzhj8iHgJ+FcLtZi1prFrfPfcWp80Fnz33FqfeFfHUnLwLaWS7cxHgD8Al0uak/TD+mWZ1VXyKQsb2yjErE2+1LGUHHxLycG3lBx8Sylts/mmTZsaXa9GI3fTM2SvuuqqRteDwRzZJrU1H9lnfEvJwbeUHHxLycG3lBx8S8nBt5RKHlJbJelFSfsk7ZV0WxuFmdVUso9/AvhJROyWdD6wS9LzEfFm5drMqinpuX0nInYPvz8O7MNzbq3nlnWNL2kSuBJ4pUYxZm0pDr6k84DHgc0R8e4iP3ezufVGUfAlnc0g9A9HxBOLHeNmc+uTkl0dAfcD+yLi3volmdVXcsZfB9wMXC1pZvj1vcp1mVVV0nP7MqAWajFrje/cWkoOvqXk4FtKDr6l1Iue26Z7TwGmp6cbXa9GjU2rMYx5+/btja539913N7reUnzGt5QcfEvJwbeUHHxLycG3lBx8S8nBt5RKHkv+rKQ/SdozbDb/WRuFmdVUcgPrv8DVEfHesCHlZUm/jYg/Vq7NrJqSx5IDeG/48uzhV9Qsyqy20tbDMUkzwGHg+YhY0Gzunlvrk6LgR8SHEbEamADWSvrqIse459Z6Y1m7OhFxDNgBrK9SjVlLSnZ1VkoaH37/OeDbwFu1CzOrqWRX50LgQUljDH5Rfh0RT9cty6yukl2d1xl8eprZp4bv3FpKDr6l5OBbSg6+pdSLZvMaA55rDDu2/vAZ31Jy8C0lB99ScvAtJQffUnLwLaXlDH8bk/SaJD+gZr23nDP+bQxm3Jr1Xmnr4QRwHbCtbjlm7Sg9428Bbgc+WuoA99xan5R0YF0PHI6IXac7zj231iel4z5vkDQLPMpg7OdDVasyq+yMwY+IOyNiIiImgQ3ACxFxU/XKzCryPr6ltKzHkiNiB4OPFzHrNZ/xLSUH31Jy8C0lB99S6kXP7eTkZONrzs7ONr5mRseOHeu6hE/EZ3xLycG3lBx8S8nBt5QcfEvJwbeUirYzh48kHwc+BE5ExJqaRZnVtpx9/G9FxNFqlZi1yJc6llJp8AP4naRdkm6pWZBZG0ovddZFxCFJXwSel/RWRLw0/4DhL8QtABdffHHDZZo1q3TA86Hhn4eBJ4G1ixzjZnPrjZJPWfi8pPNPfg98F3ijdmFmNZVc6nwJeFLSyeN/FRHPVq3KrLKSObcHgK+1UItZa7ydaSk5+JaSg28pOfiWkoNvKfWi2Xx6errxNVevXt3oejUGRjfdZF9jUPb4+Hjja7bBZ3xLycG3lBx8S8nBt5QcfEvJwbeUSsd9jkt6TNJbkvZJ+kbtwsxqKt3H3wo8GxHfl3QOcG7FmsyqO2PwJV0AfBOYBoiID4AP6pZlVlfJpc5XgCPALyW9JmnbsBPrYzzg2fqkJPhnAV8HfhERVwL/Bu449SD33FqflAR/DpiLiFeGrx9j8Itg1lslA57/ARyUdPnwrWuAN6tWZVZZ6a7Oj4CHhzs6B4Af1CvJrL6i4EfEDOAPirVPDd+5tZQcfEvJwbeUHHxLqRc9tzVMTU01ul6NvuCmhyfXGGpdo4+3DT7jW0oOvqXk4FtKDr6l5OBbSg6+pVQyCuhySTPzvt6VtLmN4sxqKZmI8jawGkDSGPB3BgPgzHpruZc61wB/jYi/1SjGrC3LDf4G4JEahZi1qTj4wyaUG4DfLPFzN5tbbyznjH8tsDsi/rnYD91sbn2ynOBvxJc59ilR+hGC5wLfAZ6oW45ZO0p7bv8DfKFyLWat8Z1bS8nBt5QcfEvJwbeUHHxLSRHR/KLSEaDkeZ4VwNHGC2jWqNc46vVBtzV+OSIW3FGtEvxSknZGxEh/NOGo1zjq9cFo1uhLHUvJwbeUug7+fR3//SVGvcZRrw9GsMZOr/HNutL1Gd+sE50EX9J6SW9L2i9pwSC5rklaJenF4TDrvZJu67qmpUgaG06jfLrrWhYzqsPBW7/UGTas/4XBY85zwKvAxogYmblaki4ELoyI3ZLOB3YBU6NU40mSfsxgWs0FEXF91/WcStKDwO8jYtvJ4eAR0eyn4X4CXZzx1wL7I+LAcFj0o8CNHdSxpIh4JyJ2D78/DuwDLuq2qoUkTQDXAdu6rmUx84aD3w+D4eCjEHroJvgXAQfnvZ5jBEN1kqRJ4ErgldMf2YktwO3AR10XsoSi4eBd6CL4WuS9kdxaknQe8DiwOSLe7bqe+SRdDxyOiF1d13IaRcPBu9BF8OeAVfNeTwCHOqjjtCSdzSD0D0fEKLZcrgNukDTL4HLxakkPdVvSAiM7HLyL4L8KXCbpkuF/djYAT3VQx5IkicF16b6IuLfrehYTEXdGxERETDL4N3whIm7quKyPGeXh4K2PAoqIE5JuBZ4DxoAHImJv23WcwTrgZuDPkk7OuvlpRDzTYU19NZLDwX3n1lLynVtLycG3lBx8S8nBt5QcfEvJwbeUHHxLycG3lP4Hx0mwgqmZiz0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input 6\n",
      "\tx=[0.   0.   0.06 0.56 1.   0.38 0.   0.   0.   0.25 0.88 0.62 0.69 0.62\n",
      " 0.   0.   0.   0.75 0.62 0.   0.81 0.38 0.   0.   0.   0.38 0.44 0.25\n",
      " 1.   0.31 0.   0.   0.   0.   0.   0.   0.44 0.75 0.06 0.   0.   0.\n",
      " 0.   0.   0.   0.5  0.62 0.   0.   0.   0.5  0.81 0.19 0.   0.88 0.19\n",
      " 0.   0.   0.   0.5  1.   1.   0.81 0.19]\n",
      "\ty=[0.   0.   0.01 0.99 0.   0.   0.   0.   0.   0.2 ], y_label=tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "\tw=[[  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.    -5.27   0.\n",
      "    0.     0.     0.     0.     0.     0.    -4.66   0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.    12.76   0.     0.     0.     0.     0.    10.3\n",
      "    0.     0.     0.     0.     0.     0.     0.     4.82   0.     0.\n",
      "    0.     0.     0.     0.     0.    -4.34   0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.    -0.38   0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.   -31.06\n",
      "    0.     0.     0.     9.8 ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.    -9.73   3.65   0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.    19.03 -16.01   0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.    -2.31   0.  ]\n",
      " [  0.     0.     0.     0.     4.49   0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.    12.27\n",
      "    0.     9.16   0.     0.     0.     0.   -12.38   0.     0.    -6.59\n",
      "   -0.03   0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.   -19.07   0.     0.    -1.02   0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.    -2.6    0.     0.    -3.52   0.     0.     0.\n",
      "    0.     0.    -1.75   0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    3.51   0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     2.24   0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.    -6.19   0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.  ]\n",
      " [  0.     0.     5.29   0.     0.     1.06   0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "   -1.57  -2.84  -0.75   0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "   -5.75   0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "  -12.01 -16.64   0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.    13.77   0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     2.02   0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     3.49   0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.    -4.58   0.     0.\n",
      "   12.81   0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.    -9.09   0.     0.     0.     0.     0.     0.\n",
      "    0.    -9.65   0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     5.12   0.     0.     0.     0.     0.     5.92   0.     0.\n",
      "    0.     0.     0.     0.     1.73   0.     0.     0.     0.     0.\n",
      "    0.     0.     4.89   5.81   0.     0.     0.     0.     0.     0.\n",
      "   -5.59   0.     0.     0.     0.     0.     0.     0.     1.46   0.\n",
      "   -3.4    0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.    12.28   0.     0.     0.     0.     0.     9.24   0.     0.\n",
      "    0.     0.     0.    -7.28   0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.   -13.22  -5.21   0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.  ]]\n",
      "\tb=[-10.04  -5.39   8.63   5.15  -2.47  -3.93  -5.92  -1.88  -5.03  -8.3 ]\n",
      "\tExplanation: feature0000000004 & ~feature0000000019 & ~feature0000000021 & ~feature0000000026 & ~feature0000000029 & ~feature0000000030 & ~feature0000000043 & feature0000000046\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input 7\n",
      "\tx=[0.   0.   0.44 0.88 0.94 0.44 0.   0.   0.   0.38 1.   0.5  0.44 1.\n",
      " 0.25 0.   0.   0.69 0.38 0.06 0.62 0.88 0.06 0.   0.   0.06 0.   0.25\n",
      " 1.   0.38 0.   0.   0.   0.   0.   0.12 0.69 0.81 0.06 0.   0.   0.\n",
      " 0.   0.   0.   0.69 0.44 0.   0.   0.   0.19 0.25 0.5  0.88 0.19 0.\n",
      " 0.   0.   0.62 0.81 0.75 0.25 0.   0.  ]\n",
      "\ty=[0.   0.   0.01 1.   0.   0.01 0.   0.   0.   0.99], y_label=tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "\tw=[[  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.    -5.27   0.\n",
      "    0.     0.     0.     0.     0.     0.    -4.66   0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     8.98   0.     0.     0.     0.     0.     7.28\n",
      "    0.     0.     0.     0.     0.     0.     0.     3.27   0.     0.\n",
      "    0.     0.     0.     0.     0.    -2.94   0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.    -0.09   0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.   -22.24\n",
      "    0.     0.     0.     7.08]\n",
      " [  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.   -13.03  10.24   0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.    27.38 -11.24   0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.    20.13   0.  ]\n",
      " [  0.     0.     0.     0.     2.86   0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.58\n",
      "    0.     2.1    0.     0.     0.     0.    -1.14   0.     0.     0.2\n",
      "   -0.01   0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.    -7.63   0.     0.     0.89   0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.    -6.58   0.     0.    -9.08   0.     0.     0.\n",
      "    0.     0.    -4.34   0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    9.04   0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     5.85   0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.   -16.56   0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.  ]\n",
      " [  0.     0.     5.16   0.     0.     0.71   0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "   -1.2   -2.52  -0.75   0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "   -4.7    0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "   -9.75 -13.54   0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.    11.16   0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     1.6    0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     3.49   0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.    -4.58   0.     0.\n",
      "   12.81   0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.    -9.09   0.     0.     0.     0.     0.     0.\n",
      "    0.    -9.65   0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.97   0.     0.     0.     0.     0.     7.5    0.     0.\n",
      "    0.     0.     0.     0.     6.89   0.     0.     0.     0.     0.\n",
      "    0.     0.     6.81   2.32   0.     0.     0.     0.     0.     0.\n",
      "   -2.37   0.     0.     0.     0.     0.     0.     0.    -3.99   0.\n",
      "   -1.16   0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.    12.82   0.     0.     0.     0.     0.     9.59   0.     0.\n",
      "    0.     0.     0.    -7.44   0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.   -13.7   -5.3    0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.  ]]\n",
      "\tb=[-10.04  -7.95   0.85   8.44   4.6   -4.22  -9.77  -1.88  -5.19  -8.45]\n",
      "\tExplanation: feature0000000004 & ~feature0000000019 & feature0000000021 & ~feature0000000026 & ~feature0000000029 & ~feature0000000030 & ~feature0000000043 & ~feature0000000046\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input 8\n",
      "\tx=[0.   0.06 0.62 0.88 0.81 0.25 0.   0.   0.   0.75 0.69 0.31 0.5  0.88\n",
      " 0.   0.   0.   0.5  0.19 0.12 0.75 0.5  0.   0.   0.   0.   0.19 0.94\n",
      " 0.94 0.25 0.   0.   0.   0.   0.06 0.25 0.44 0.88 0.31 0.   0.   0.\n",
      " 0.   0.   0.   0.44 0.75 0.   0.   0.   0.   0.   0.06 0.69 0.69 0.\n",
      " 0.   0.   0.75 1.   1.   0.56 0.06 0.  ]\n",
      "\ty=[0.   0.   0.97 1.   0.   0.03 0.   0.   0.01 0.99], y_label=tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "\tw=[[  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.    -5.27   0.\n",
      "    0.     0.     0.     0.     0.     0.    -4.66   0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     4.78   0.     0.     0.     0.     0.     2.54\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.74   0.     0.\n",
      "    0.     0.     0.     0.     0.    -0.68   0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.    -0.53   0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.   -13.33\n",
      "    0.     0.     0.     7.11]\n",
      " [  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.    -9.9    2.56   0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     9.39 -13.76   0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     3.2    0.  ]\n",
      " [  0.     0.     0.     0.     4.49   0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.    12.27\n",
      "    0.     9.16   0.     0.     0.     0.   -12.38   0.     0.    -6.59\n",
      "   -0.03   0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.   -19.07   0.     0.    -1.02   0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.    -2.6    0.     0.    -3.52   0.     0.     0.\n",
      "    0.     0.    -1.75   0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    3.51   0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     2.24   0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.    -6.19   0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.  ]\n",
      " [  0.     0.     1.94   0.     0.     2.05   0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "   -2.25  -1.59  -0.89   0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "   -5.16   0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "  -10.71 -14.87   0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.    12.27   0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     1.76   0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     3.49   0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.    -4.58   0.     0.\n",
      "   12.81   0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.    -9.09   0.     0.     0.     0.     0.     0.\n",
      "    0.    -9.65   0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.    -1.01   0.     0.     0.     0.     0.     3.77   0.     0.\n",
      "    0.     0.     0.     0.     5.56   0.     0.     0.     0.     0.\n",
      "    0.     0.    14.93  -9.41   0.     0.     0.     0.     0.     0.\n",
      "   23.73   0.     0.     0.     0.     0.     0.     0.   -18.01   0.\n",
      "    3.08   0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.    15.32   0.     0.     0.     0.     0.     5.76   0.     0.\n",
      "    0.     0.     0.    -8.17   0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.   -12.37  -4.66   0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.  ]]\n",
      "\tb=[-10.04 -10.04   8.72   5.15  -2.47  -2.68  -7.9   -1.88   2.21  -8.37]\n",
      "\tExplanation: feature0000000004 & ~feature0000000019 & ~feature0000000021 & ~feature0000000026 & ~feature0000000029 & ~feature0000000030 & ~feature0000000043 & feature0000000046\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input 9\n",
      "\tx=[0.   0.   0.44 0.88 0.5  0.25 0.   0.   0.   0.   1.   0.5  0.94 0.88\n",
      " 0.06 0.   0.   0.25 1.   0.25 0.   0.5  0.25 0.   0.   0.5  0.88 0.\n",
      " 0.   0.25 0.25 0.   0.   0.5  1.   0.   0.   0.25 0.31 0.   0.   0.19\n",
      " 1.   0.06 0.   0.69 0.25 0.   0.   0.   0.94 1.   1.   0.75 0.   0.\n",
      " 0.   0.   0.38 0.81 0.44 0.   0.   0.  ]\n",
      "\ty=[1.   0.   0.   0.   0.32 0.42 0.43 0.49 0.19 0.01], y_label=tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "\tw=[[  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.   -29.57   0.\n",
      "    0.     0.     0.     0.     0.     0.   -26.16   0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     7.16   0.     0.     0.     0.     0.     5.89\n",
      "    0.     0.     0.     0.     0.     0.     0.     2.45   0.     0.\n",
      "    0.     0.     0.     0.     0.    -2.23   0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.    -0.58   0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.   -18.11\n",
      "    0.     0.     0.     5.91]\n",
      " [  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.   -13.03  10.24   0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.    27.38 -11.24   0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.    20.13   0.  ]\n",
      " [  0.     0.     0.     0.     6.19   0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     1.49\n",
      "    0.     1.3    0.     0.     0.     0.   -10.5    0.     0.    -9.72\n",
      "  -13.11   0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.   -10.58   0.     0.     5.68   0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.    -8.38   0.     0.   -11.6    0.     0.     0.\n",
      "    0.     0.    -5.51   0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "   11.53   0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     7.48   0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.   -21.24   0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.  ]\n",
      " [  0.     0.    18.36   0.     0.    10.67   0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "  -14.97 -16.05  -9.23   0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "   -7.82   0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "  -16.59 -22.84   0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.    19.03   0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     2.94   0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     5.88   0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.    -6.36   0.     0.\n",
      "   19.43   0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.   -13.79   0.     0.     0.     0.     0.     0.\n",
      "    0.   -14.77   0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.    11.24   0.     0.     0.     0.     0.     9.98   0.     0.\n",
      "    0.     0.     0.     0.    -4.89   0.     0.     0.     0.     0.\n",
      "    0.     0.    -7.37  -2.39   0.     0.     0.     0.     0.     0.\n",
      "   -4.5    0.     0.     0.     0.     0.     0.     0.     3.77   0.\n",
      "   -5.66   0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     3.6    0.     0.     0.     0.     0.     2.74   0.     0.\n",
      "    0.     0.     0.    -2.17   0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.    -3.93  -1.55   0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.  ]]\n",
      "\tb=[10.49 -9.18  0.85  1.51  7.45 -0.67 -0.09 -0.06 10.63 -4.95]\n",
      "\tExplanation: ~feature0000000028 & ~feature0000000036\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input 10\n",
      "\tx=[0.   0.   0.   0.25 0.81 1.   0.94 0.12 0.   0.   0.12 0.94 0.81 0.81\n",
      " 1.   0.38 0.   0.   0.44 0.44 0.   0.19 1.   0.25 0.   0.   0.   0.25\n",
      " 0.25 0.5  0.88 0.   0.   0.   0.88 1.   1.   1.   0.38 0.   0.   0.\n",
      " 0.69 0.56 0.62 0.75 0.   0.   0.   0.   0.   0.   0.81 0.19 0.   0.\n",
      " 0.   0.   0.   0.25 0.62 0.   0.   0.  ]\n",
      "\ty=[0.   0.02 0.63 0.   0.52 0.13 0.26 1.   0.   0.  ], y_label=tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "\tw=[[  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.    -5.27   0.\n",
      "    0.     0.     0.     0.     0.     0.    -4.66   0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     2.61   0.     0.     0.     0.     0.     9.99\n",
      "    0.     0.     0.     0.     0.     0.     0.     4.     0.     0.\n",
      "    0.     0.     0.     0.     0.    -1.93   0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.   -37.37   0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     4.64\n",
      "    0.     0.     0.    11.49]\n",
      " [  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.    -4.29  -3.99   0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.   -12.87   2.89   0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.    34.61   0.  ]\n",
      " [  0.     0.     0.     0.     2.7    0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.    -1.26\n",
      "    0.    -0.06   0.     0.     0.     0.    -2.51   0.     0.    -3.05\n",
      "   -5.72   0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.    -4.     0.     0.     2.97   0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.    -8.38   0.     0.   -11.6    0.     0.     0.\n",
      "    0.     0.    -5.51   0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "   11.53   0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     7.48   0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.   -21.24   0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.  ]\n",
      " [  0.     0.    10.75   0.     0.     5.96   0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "   -8.42  -9.22  -5.19   0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    7.31   0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "  -12.99   5.71   0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     5.52   0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.    21.28   0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     2.67   0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.    -2.13   0.     0.\n",
      "    7.64   0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.    -5.42   0.     0.     0.     0.     0.     0.\n",
      "    0.    -5.88   0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     2.96   0.     0.     0.     0.     0.    -1.38   0.     0.\n",
      "    0.     0.     0.     0.    -3.87   0.     0.     0.     0.     0.\n",
      "    0.     0.    -3.38  -5.51   0.     0.     0.     0.     0.     0.\n",
      "    8.95   0.     0.     0.     0.     0.     0.     0.    -1.12   0.\n",
      "   -0.45   0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     2.96   0.     0.     0.     0.     0.     2.34   0.     0.\n",
      "    0.     0.     0.    -1.97   0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.    -2.47  -1.47   0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.  ]]\n",
      "\tb=[-10.04 -10.51   7.38  -5.71   7.45  -0.91  -6.84   0.6    2.86  -4.87]\n",
      "\tExplanation: feature0000000012 & ~feature0000000027 & feature0000000030 & ~feature0000000053 & ~feature0000000061\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input 11\n",
      "\tx=[0.   0.   0.06 0.56 1.   1.   1.   0.06 0.   0.   0.5  0.94 0.62 0.88\n",
      " 0.81 0.   0.   0.   0.88 0.38 0.   0.88 0.38 0.   0.   0.   0.38 0.12\n",
      " 0.38 1.   0.19 0.   0.   0.   0.   0.75 1.   1.   0.62 0.   0.   0.\n",
      " 0.   0.38 1.   0.5  0.   0.   0.   0.   0.   0.44 0.81 0.   0.   0.\n",
      " 0.   0.   0.   0.88 0.38 0.   0.   0.  ]\n",
      "\ty=[0.   0.   0.   0.   0.   0.02 0.   0.97 0.14 0.01], y_label=tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "\tw=[[  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.    -5.27   0.\n",
      "    0.     0.     0.     0.     0.     0.    -4.66   0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.    -1.28   0.     0.     0.     0.     0.    20.06\n",
      "    0.     0.     0.     0.     0.     0.     0.    -0.44   0.     0.\n",
      "    0.     0.     0.     0.     0.     4.4    0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.   -32.51   0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.    -6.22\n",
      "    0.     0.     0.     9.82]\n",
      " [  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.    -4.55  -4.06   0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.   -13.77   3.5    0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.    37.92   0.  ]\n",
      " [  0.     0.     0.     0.     4.07   0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.67\n",
      "    0.     0.63   0.     0.     0.     0.    -6.91   0.     0.    -6.57\n",
      "   -8.57   0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.    -6.89   0.     0.     3.54   0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.    -2.6    0.     0.    -3.52   0.     0.     0.\n",
      "    0.     0.    -1.75   0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    3.51   0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     2.24   0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.    -6.19   0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.  ]\n",
      " [  0.     0.     6.1    0.     0.     3.17   0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "   -4.53  -5.11  -2.79   0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "   -5.75   0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "  -12.01 -16.64   0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.    13.77   0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     2.02   0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     2.67   0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.    -2.13   0.     0.\n",
      "    7.64   0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.    -5.42   0.     0.     0.     0.     0.     0.\n",
      "    0.    -5.88   0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.    -1.8    0.     0.     0.     0.     0.     5.75   0.     0.\n",
      "    0.     0.     0.     0.     7.96   0.     0.     0.     0.     0.\n",
      "    0.     0.    14.83  -9.85   0.     0.     0.     0.     0.     0.\n",
      "   22.39   0.     0.     0.     0.     0.     0.     0.   -18.01   0.\n",
      "    2.87   0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     2.96   0.     0.     0.     0.     0.     2.34   0.     0.\n",
      "    0.     0.     0.    -1.97   0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.    -2.47  -1.47   0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.  ]]\n",
      "\tb=[-10.04 -10.17   8.    -1.64  -2.47  -1.73  -5.92   0.6    1.68  -4.87]\n",
      "\tExplanation: feature0000000012 & ~feature0000000027 & ~feature0000000030 & ~feature0000000053 & ~feature0000000061\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input 12\n",
      "\tx=[0.   0.   0.56 0.5  0.75 0.81 0.06 0.   0.   0.19 0.94 0.5  0.31 0.25\n",
      " 0.   0.   0.   0.38 0.56 0.12 0.38 0.12 0.   0.   0.   0.38 1.   0.88\n",
      " 0.56 0.81 0.25 0.   0.   0.12 0.44 0.   0.   0.44 0.5  0.   0.   0.\n",
      " 0.   0.   0.   0.44 0.62 0.   0.   0.   0.5  0.31 0.38 0.88 0.19 0.\n",
      " 0.   0.   0.62 0.88 0.94 0.31 0.   0.  ]\n",
      "\ty=[0.   0.   0.   0.   0.43 1.   0.   0.   0.06 0.08], y_label=tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "\tw=[[  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.   -29.57   0.\n",
      "    0.     0.     0.     0.     0.     0.   -26.16   0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     4.13   0.     0.     0.     0.     0.     3.56\n",
      "    0.     0.     0.     0.     0.     0.     0.     1.19   0.     0.\n",
      "    0.     0.     0.     0.     0.    -1.09   0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.    -0.76   0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.   -11.05\n",
      "    0.     0.     0.     3.79]\n",
      " [  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.   -13.35   8.25   0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     9.93  -5.12   0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.    31.82   0.  ]\n",
      " [  0.     0.     0.     0.     3.99   0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     1.07\n",
      "    0.     0.89   0.     0.     0.     0.    -6.92   0.     0.    -6.4\n",
      "   -8.57   0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.    -6.96   0.     0.     3.69   0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.    -8.38   0.     0.   -11.6    0.     0.     0.\n",
      "    0.     0.    -5.51   0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "   11.53   0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     7.48   0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.   -21.24   0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.  ]\n",
      " [  0.     0.     5.08   0.     0.     8.34   0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "  -10.22  -7.48  -6.46   0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "   -7.82   0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "  -16.59 -22.84   0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.    19.03   0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     2.94   0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     3.49   0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.    -4.58   0.     0.\n",
      "   12.81   0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.    -9.09   0.     0.     0.     0.     0.     0.\n",
      "    0.    -9.65   0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.    14.68   0.     0.     0.     0.     0.    12.5    0.     0.\n",
      "    0.     0.     0.     0.    -1.77   0.     0.     0.     0.     0.\n",
      "    0.     0.    12.46  11.28   0.     0.     0.     0.     0.     0.\n",
      "   -9.64   0.     0.     0.     0.     0.     0.     0.     1.89   0.\n",
      "   -7.44   0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.    14.59   0.     0.     0.     0.     0.     5.7    0.     0.\n",
      "    0.     0.     0.    -7.96   0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.   -11.99  -4.62   0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "    0.     0.     0.     0.  ]]\n",
      "\tb=[ 10.49 -12.19  -0.91  -2.     7.45   2.27  -0.09  -1.88  -4.18  -8.22]\n",
      "\tExplanation: feature0000000002 & feature0000000005 & ~feature0000000020 & ~feature0000000021 & ~feature0000000022\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %% md\n",
    "\n",
    "# Local explanations\n",
    "\n",
    "# %%\n",
    "\n",
    "np.set_printoptions(precision=2, suppress=True)\n",
    "outputs = []\n",
    "for i, (xin, yin) in enumerate(zip(x_train, y_train)):\n",
    "    model_reduced = get_reduced_model(model, xin)\n",
    "    for module in model_reduced.children():\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            wa = module.weight.detach().numpy()\n",
    "            ba = module.bias.detach().numpy()\n",
    "            break\n",
    "    output = model_reduced(xin)\n",
    "\n",
    "    pred_class = torch.argmax(output)\n",
    "    true_class = torch.argmax(y_train[i])\n",
    "\n",
    "    # generate local explanation only if the prediction is correct\n",
    "    if pred_class.eq(true_class):\n",
    "        local_explanation = fol.relunn.explain_local(model, x_train, y_train, xin)\n",
    "        print(f'Input {(i + 1)}')\n",
    "        print(f'\\tx={xin.detach().numpy()}')\n",
    "        print(f'\\ty={output.detach().numpy()}, y_label={yin}')\n",
    "        print(f'\\tw={wa}')\n",
    "        print(f'\\tb={ba}')\n",
    "        print(f'\\tExplanation: {local_explanation}')\n",
    "        print()\n",
    "        xin = xin.reshape(8, 8)\n",
    "        plt.figure(1, figsize=(3, 3))\n",
    "        plt.imshow(xin, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "        plt.show()\n",
    "#         wa = wa.reshape(8, 8)\n",
    "#         plt.figure(1, figsize=(3, 3))\n",
    "#         plt.imshow(wa * xin.numpy(), cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "#         plt.show()\n",
    "\n",
    "    outputs.append(output)\n",
    "    if i > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0 - Global explanation: \"~f_28 & ~f_36\" - Accuracy: 0.9177 - F1: 0.7152\n",
      "Class 1 - Global explanation: \"(f_19 & f_27 & f_35 & ~f_46 & ~f_63) | (f_19 & f_27 & f_59 & ~f_13 & ~f_46) | (f_13 & f_27 & f_35 & ~f_46 & ~f_59 & ~f_63) | (f_13 & f_19 & ~f_35 & ~f_46 & ~f_59 & ~f_63)\" - Accuracy: 0.8621 - F1: 0.5863\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-b3d25ecf7151>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m                                                                               \u001b[0mtopk_explanations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m                                                                               \u001b[0mtarget_class\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtarget_class\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m                                                                               concept_names=concept_names)\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0my2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mtarget_class\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Scuola\\Dottorato\\Codice\\InterpretableReLU\\deep_logic\\fol\\relunn.py\u001b[0m in \u001b[0;36mcombine_local_explanations\u001b[1;34m(model, x, y, target_class, topk_explanations, concept_names, device)\u001b[0m\n\u001b[0;32m    169\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0msample_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mxi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m         \u001b[1;31m# get prediction for each sample\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 171\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    172\u001b[0m         \u001b[0mpred_class\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m         \u001b[0mtrue_class\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fsc\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fsc\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fsc\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    717\u001b[0m                 \u001b[0m_global_forward_pre_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    718\u001b[0m                 self._forward_pre_hooks.values()):\n\u001b[1;32m--> 719\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    720\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fsc\\lib\\site-packages\\torch\\nn\\utils\\prune.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, module, inputs)\u001b[0m\n\u001b[0;32m     30\u001b[0m             \u001b[0minputs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mused\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \"\"\"\n\u001b[1;32m---> 32\u001b[1;33m         \u001b[0msetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tensor_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fsc\\lib\\site-packages\\torch\\nn\\utils\\prune.py\u001b[0m in \u001b[0;36mapply_mask\u001b[1;34m(self, module)\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tensor_name\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"_mask\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[0morig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tensor_name\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"_orig\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m         \u001b[0mpruned_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0morig\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mpruned_tensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# %% md\n",
    "\n",
    "# Combine local explanations\n",
    "\n",
    "# %%\n",
    "counters = []\n",
    "from sklearn.metrics import f1_score\n",
    "y_train_d = torch.argmax(y_train, dim=1)\n",
    "for target_class in range(n_classes):\n",
    "    global_explanation, predictions, counter = fol.combine_local_explanations(model, x_train, y_train,\n",
    "                                                                              topk_explanations=10,\n",
    "                                                                              target_class=target_class,\n",
    "                                                                              concept_names=concept_names)\n",
    "\n",
    "    y2 = torch.argmax(y_train, dim=1) == target_class\n",
    "    accuracy = sum(predictions == y2.detach().numpy().squeeze()) / len(predictions)\n",
    "    f1 = f1_score(y_train[:, target_class], predictions)\n",
    "    print(f'Class {target_class} - Global explanation: \"{global_explanation}\" - Accuracy: {accuracy:.4f} - F1: {f1:.4f}')\n",
    "    counters.append(counter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 ('~f_28 & ~f_36', 165)\n",
      "1 0 ('~f_13 & f_19 & f_27 & f_35 & ~f_46 & f_59 & ~f_63', 58)\n",
      "1 1 ('f_13 & f_19 & ~f_27 & ~f_35 & ~f_46 & ~f_59 & ~f_63', 11)\n",
      "1 2 ('f_13 & f_19 & f_27 & ~f_35 & ~f_46 & ~f_59 & ~f_63', 21)\n",
      "1 3 ('f_13 & f_19 & f_27 & f_35 & ~f_46 & ~f_59 & ~f_63', 16)\n",
      "1 4 ('~f_13 & f_19 & f_27 & f_35 & ~f_46 & f_59 & f_63', 10)\n",
      "1 5 ('~f_13 & f_19 & f_27 & ~f_35 & ~f_46 & f_59 & ~f_63', 2)\n",
      "1 6 ('~f_13 & f_19 & f_27 & ~f_35 & ~f_46 & f_59 & f_63', 5)\n",
      "1 7 ('f_13 & ~f_19 & f_27 & f_35 & ~f_46 & ~f_59 & ~f_63', 2)\n",
      "1 8 ('f_13 & f_19 & f_27 & f_35 & ~f_46 & f_59 & ~f_63', 25)\n",
      "1 9 ('~f_13 & f_19 & f_27 & f_35 & ~f_46 & ~f_59 & ~f_63', 6)\n",
      "1 10 ('f_13 & f_19 & ~f_27 & f_35 & ~f_46 & f_59 & ~f_63', 1)\n",
      "1 11 ('~f_13 & f_19 & ~f_27 & f_35 & ~f_46 & f_59 & ~f_63', 1)\n",
      "1 12 ('f_13 & ~f_19 & f_27 & ~f_35 & ~f_46 & ~f_59 & ~f_63', 1)\n",
      "1 13 ('~f_13 & f_19 & f_27 & ~f_35 & ~f_46 & ~f_59 & ~f_63', 1)\n"
     ]
    }
   ],
   "source": [
    "for i, counter in enumerate(counters):\n",
    "    for j, values in enumerate(counter.items()):\n",
    "        print(i, j, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted'].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-6d89daa1d212>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Accuracy: {accuracy_score(y_test, y_pred):.2f}.\\n F1: {f1_score(y_test, y_pred):.2f}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\fsc\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36mf1_score\u001b[1;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1097\u001b[0m                        \u001b[0mpos_label\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpos_label\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maverage\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1098\u001b[0m                        \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1099\u001b[1;33m                        zero_division=zero_division)\n\u001b[0m\u001b[0;32m   1100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fsc\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36mfbeta_score\u001b[1;34m(y_true, y_pred, beta, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1224\u001b[0m                                                  \u001b[0mwarn_for\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'f-score'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1225\u001b[0m                                                  \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1226\u001b[1;33m                                                  zero_division=zero_division)\n\u001b[0m\u001b[0;32m   1227\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1228\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fsc\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36mprecision_recall_fscore_support\u001b[1;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1482\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"beta should be >=0 in the F-beta score\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1483\u001b[0m     labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n\u001b[1;32m-> 1484\u001b[1;33m                                     pos_label)\n\u001b[0m\u001b[0;32m   1485\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1486\u001b[0m     \u001b[1;31m# Calculate tp_sum, pred_sum, true_sum ###\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fsc\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36m_check_set_wise_labels\u001b[1;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[0;32m   1314\u001b[0m             raise ValueError(\"Target is %s but average='binary'. Please \"\n\u001b[0;32m   1315\u001b[0m                              \u001b[1;34m\"choose another average setting, one of %r.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1316\u001b[1;33m                              % (y_type, average_options))\n\u001b[0m\u001b[0;32m   1317\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mpos_label\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m         warnings.warn(\"Note that pos_label (set to %r) is ignored when \"\n",
      "\u001b[1;31mValueError\u001b[0m: Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted']."
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "y_pred = model(torch.Tensor(X_test)).argmax(dim=1).detach().numpy()\n",
    "y_test = np.argmax(y_test, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.84.\n",
      "F1: 0.8367041408858945\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "print(f\"Accuracy: {accuracy:.2f}.\\nF1: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.85.\n",
      "F1: 0.85\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "tree_model = DecisionTreeClassifier()\n",
    "X_bool = X > 0.5\n",
    "tree_model.fit(X, y_train_d)\n",
    "X_test_bool = X_test > 0.5\n",
    "y_pred = tree_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "print(f\"Accuracy: {accuracy:.2f}.\\nF1: {f1:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(93.55618261455527, 209.07692307692307, 'X[36] <= 0.031\\ngini = 0.9\\nsamples = 1617\\nvalue = [167, 162, 161, 173, 171, 161, 156, 159, 151, 156]'),\n",
       " Text(22.560646900269543, 192.35076923076923, 'X[28] <= 0.281\\ngini = 0.535\\nsamples = 252\\nvalue = [163, 0, 3, 1, 6, 22, 5, 0, 3, 49]'),\n",
       " Text(12.633962264150945, 175.62461538461537, 'X[21] <= 0.031\\ngini = 0.187\\nsamples = 181\\nvalue = [163, 0, 2, 0, 6, 5, 3, 0, 0, 2]'),\n",
       " Text(9.024258760107816, 158.89846153846153, 'X[5] <= 0.656\\ngini = 0.75\\nsamples = 16\\nvalue = [1, 0, 2, 0, 5, 5, 3, 0, 0, 0]'),\n",
       " Text(7.219407008086254, 142.1723076923077, 'X[37] <= 0.344\\ngini = 0.678\\nsamples = 11\\nvalue = [1, 0, 2, 0, 5, 0, 3, 0, 0, 0]'),\n",
       " Text(5.41455525606469, 125.44615384615385, 'X[51] <= 0.969\\ngini = 0.611\\nsamples = 6\\nvalue = [1, 0, 2, 0, 0, 0, 3, 0, 0, 0]'),\n",
       " Text(3.609703504043127, 108.72, 'X[45] <= 0.312\\ngini = 0.444\\nsamples = 3\\nvalue = [1, 0, 2, 0, 0, 0, 0, 0, 0, 0]'),\n",
       " Text(1.8048517520215634, 91.99384615384615, 'gini = 0.0\\nsamples = 2\\nvalue = [0, 0, 2, 0, 0, 0, 0, 0, 0, 0]'),\n",
       " Text(5.41455525606469, 91.99384615384615, 'gini = 0.0\\nsamples = 1\\nvalue = [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]'),\n",
       " Text(7.219407008086254, 108.72, 'gini = 0.0\\nsamples = 3\\nvalue = [0, 0, 0, 0, 0, 0, 3, 0, 0, 0]'),\n",
       " Text(9.024258760107816, 125.44615384615385, 'gini = 0.0\\nsamples = 5\\nvalue = [0, 0, 0, 0, 5, 0, 0, 0, 0, 0]'),\n",
       " Text(10.82911051212938, 142.1723076923077, 'gini = 0.0\\nsamples = 5\\nvalue = [0, 0, 0, 0, 0, 5, 0, 0, 0, 0]'),\n",
       " Text(16.24366576819407, 158.89846153846153, 'X[50] <= 0.281\\ngini = 0.036\\nsamples = 165\\nvalue = [162, 0, 0, 0, 1, 0, 0, 0, 0, 2]'),\n",
       " Text(14.438814016172508, 142.1723076923077, 'X[44] <= 0.438\\ngini = 0.444\\nsamples = 3\\nvalue = [0, 0, 0, 0, 1, 0, 0, 0, 0, 2]'),\n",
       " Text(12.633962264150945, 125.44615384615385, 'gini = 0.0\\nsamples = 2\\nvalue = [0, 0, 0, 0, 0, 0, 0, 0, 0, 2]'),\n",
       " Text(16.24366576819407, 125.44615384615385, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 0, 0, 0, 1, 0, 0, 0, 0, 0]'),\n",
       " Text(18.048517520215633, 142.1723076923077, 'gini = 0.0\\nsamples = 162\\nvalue = [162, 0, 0, 0, 0, 0, 0, 0, 0, 0]'),\n",
       " Text(32.48733153638814, 175.62461538461537, 'X[21] <= 0.469\\ngini = 0.501\\nsamples = 71\\nvalue = [0, 0, 1, 1, 0, 17, 2, 0, 3, 47]'),\n",
       " Text(25.26792452830189, 158.89846153846153, 'X[42] <= 0.344\\ngini = 0.404\\nsamples = 21\\nvalue = [0, 0, 1, 0, 0, 16, 2, 0, 1, 1]'),\n",
       " Text(21.65822102425876, 142.1723076923077, 'X[11] <= 0.969\\ngini = 0.111\\nsamples = 17\\nvalue = [0, 0, 0, 0, 0, 16, 0, 0, 0, 1]'),\n",
       " Text(19.8533692722372, 125.44615384615385, 'gini = 0.0\\nsamples = 16\\nvalue = [0, 0, 0, 0, 0, 16, 0, 0, 0, 0]'),\n",
       " Text(23.463072776280324, 125.44615384615385, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]'),\n",
       " Text(28.877628032345015, 142.1723076923077, 'X[50] <= 0.562\\ngini = 0.625\\nsamples = 4\\nvalue = [0, 0, 1, 0, 0, 0, 2, 0, 1, 0]'),\n",
       " Text(27.072776280323453, 125.44615384615385, 'gini = 0.0\\nsamples = 2\\nvalue = [0, 0, 0, 0, 0, 0, 2, 0, 0, 0]'),\n",
       " Text(30.682479784366578, 125.44615384615385, 'X[43] <= 0.469\\ngini = 0.5\\nsamples = 2\\nvalue = [0, 0, 1, 0, 0, 0, 0, 0, 1, 0]'),\n",
       " Text(28.877628032345015, 108.72, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]'),\n",
       " Text(32.48733153638814, 108.72, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 0, 0, 0, 0, 0, 0, 0, 1, 0]'),\n",
       " Text(39.7067385444744, 158.89846153846153, 'X[34] <= 0.5\\ngini = 0.151\\nsamples = 50\\nvalue = [0, 0, 0, 1, 0, 1, 0, 0, 2, 46]'),\n",
       " Text(36.097035040431265, 142.1723076923077, 'X[28] <= 0.406\\ngini = 0.042\\nsamples = 47\\nvalue = [0, 0, 0, 1, 0, 0, 0, 0, 0, 46]'),\n",
       " Text(34.2921832884097, 125.44615384615385, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]'),\n",
       " Text(37.901886792452835, 125.44615384615385, 'gini = 0.0\\nsamples = 46\\nvalue = [0, 0, 0, 0, 0, 0, 0, 0, 0, 46]'),\n",
       " Text(43.31644204851752, 142.1723076923077, 'X[38] <= 0.25\\ngini = 0.444\\nsamples = 3\\nvalue = [0, 0, 0, 0, 0, 1, 0, 0, 2, 0]'),\n",
       " Text(41.51159029649596, 125.44615384615385, 'gini = 0.0\\nsamples = 2\\nvalue = [0, 0, 0, 0, 0, 0, 0, 0, 2, 0]'),\n",
       " Text(45.121293800539085, 125.44615384615385, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]'),\n",
       " Text(164.551718328841, 192.35076923076923, 'X[26] <= 0.469\\ngini = 0.888\\nsamples = 1365\\nvalue = [4, 162, 158, 172, 165, 139, 151, 159, 148, 107]'),\n",
       " Text(92.58325471698113, 175.62461538461537, 'X[43] <= 0.094\\ngini = 0.794\\nsamples = 588\\nvalue = [0, 59, 148, 163, 3, 7, 5, 122, 64, 17]'),\n",
       " Text(60.913746630727765, 158.89846153846153, 'X[19] <= 0.531\\ngini = 0.415\\nsamples = 188\\nvalue = [0, 10, 6, 142, 0, 2, 0, 1, 11, 16]'),\n",
       " Text(50.53584905660378, 142.1723076923077, 'X[28] <= 0.344\\ngini = 0.198\\nsamples = 150\\nvalue = [0, 0, 5, 134, 0, 0, 0, 1, 3, 7]'),\n",
       " Text(48.73099730458221, 125.44615384615385, 'gini = 0.0\\nsamples = 5\\nvalue = [0, 0, 5, 0, 0, 0, 0, 0, 0, 0]'),\n",
       " Text(52.34070080862534, 125.44615384615385, 'X[30] <= 0.219\\ngini = 0.143\\nsamples = 145\\nvalue = [0, 0, 0, 134, 0, 0, 0, 1, 3, 7]'),\n",
       " Text(48.73099730458221, 108.72, 'X[18] <= 0.781\\ngini = 0.083\\nsamples = 140\\nvalue = [0, 0, 0, 134, 0, 0, 0, 0, 3, 3]'),\n",
       " Text(46.92614555256065, 91.99384615384615, 'gini = 0.0\\nsamples = 129\\nvalue = [0, 0, 0, 129, 0, 0, 0, 0, 0, 0]'),\n",
       " Text(50.53584905660378, 91.99384615384615, 'X[51] <= 0.344\\ngini = 0.645\\nsamples = 11\\nvalue = [0, 0, 0, 5, 0, 0, 0, 0, 3, 3]'),\n",
       " Text(48.73099730458221, 75.2676923076923, 'X[42] <= 0.406\\ngini = 0.5\\nsamples = 6\\nvalue = [0, 0, 0, 0, 0, 0, 0, 0, 3, 3]'),\n",
       " Text(46.92614555256065, 58.541538461538465, 'gini = 0.0\\nsamples = 3\\nvalue = [0, 0, 0, 0, 0, 0, 0, 0, 0, 3]'),\n",
       " Text(50.53584905660378, 58.541538461538465, 'gini = 0.0\\nsamples = 3\\nvalue = [0, 0, 0, 0, 0, 0, 0, 0, 3, 0]'),\n",
       " Text(52.34070080862534, 75.2676923076923, 'gini = 0.0\\nsamples = 5\\nvalue = [0, 0, 0, 5, 0, 0, 0, 0, 0, 0]'),\n",
       " Text(55.95040431266847, 108.72, 'X[44] <= 0.344\\ngini = 0.32\\nsamples = 5\\nvalue = [0, 0, 0, 0, 0, 0, 0, 1, 0, 4]'),\n",
       " Text(54.145552560646905, 91.99384615384615, 'gini = 0.0\\nsamples = 4\\nvalue = [0, 0, 0, 0, 0, 0, 0, 0, 0, 4]'),\n",
       " Text(57.75525606469003, 91.99384615384615, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 0, 0, 0, 0, 0, 0, 1, 0, 0]'),\n",
       " Text(71.29164420485175, 142.1723076923077, 'X[34] <= 0.344\\ngini = 0.783\\nsamples = 38\\nvalue = [0, 10, 1, 8, 0, 2, 0, 0, 8, 9]'),\n",
       " Text(69.48679245283019, 125.44615384615385, 'X[10] <= 0.531\\ngini = 0.722\\nsamples = 30\\nvalue = [0, 10, 1, 8, 0, 2, 0, 0, 0, 9]'),\n",
       " Text(63.16981132075472, 108.72, 'X[36] <= 0.219\\ngini = 0.292\\nsamples = 12\\nvalue = [0, 10, 1, 0, 0, 0, 0, 0, 0, 1]'),\n",
       " Text(61.364959568733155, 91.99384615384615, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]'),\n",
       " Text(64.97466307277628, 91.99384615384615, 'X[45] <= 0.5\\ngini = 0.165\\nsamples = 11\\nvalue = [0, 10, 1, 0, 0, 0, 0, 0, 0, 0]'),\n",
       " Text(63.16981132075472, 75.2676923076923, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]'),\n",
       " Text(66.77951482479784, 75.2676923076923, 'gini = 0.0\\nsamples = 10\\nvalue = [0, 10, 0, 0, 0, 0, 0, 0, 0, 0]'),\n",
       " Text(75.80377358490567, 108.72, 'X[30] <= 0.094\\ngini = 0.593\\nsamples = 18\\nvalue = [0, 0, 0, 8, 0, 2, 0, 0, 0, 8]'),\n",
       " Text(73.99892183288411, 91.99384615384615, 'X[3] <= 0.844\\ngini = 0.5\\nsamples = 12\\nvalue = [0, 0, 0, 8, 0, 2, 0, 0, 0, 2]'),\n",
       " Text(70.38921832884097, 75.2676923076923, 'X[17] <= 0.031\\ngini = 0.444\\nsamples = 3\\nvalue = [0, 0, 0, 0, 0, 2, 0, 0, 0, 1]'),\n",
       " Text(68.5843665768194, 58.541538461538465, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]'),\n",
       " Text(72.19407008086253, 58.541538461538465, 'gini = 0.0\\nsamples = 2\\nvalue = [0, 0, 0, 0, 0, 2, 0, 0, 0, 0]'),\n",
       " Text(77.60862533692723, 75.2676923076923, 'X[19] <= 0.906\\ngini = 0.198\\nsamples = 9\\nvalue = [0, 0, 0, 8, 0, 0, 0, 0, 0, 1]'),\n",
       " Text(75.80377358490567, 58.541538461538465, 'gini = 0.0\\nsamples = 8\\nvalue = [0, 0, 0, 8, 0, 0, 0, 0, 0, 0]'),\n",
       " Text(79.4134770889488, 58.541538461538465, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]'),\n",
       " Text(77.60862533692723, 91.99384615384615, 'gini = 0.0\\nsamples = 6\\nvalue = [0, 0, 0, 0, 0, 0, 0, 0, 0, 6]'),\n",
       " Text(73.09649595687333, 125.44615384615385, 'gini = 0.0\\nsamples = 8\\nvalue = [0, 0, 0, 0, 0, 0, 0, 0, 8, 0]'),\n",
       " Text(124.2527628032345, 158.89846153846153, 'X[38] <= 0.031\\ngini = 0.747\\nsamples = 400\\nvalue = [0, 49, 142, 21, 3, 5, 5, 121, 53, 1]'),\n",
       " Text(103.21495956873316, 142.1723076923077, 'X[27] <= 0.656\\ngini = 0.645\\nsamples = 265\\nvalue = [0, 48, 141, 13, 0, 5, 1, 7, 50, 0]'),\n",
       " Text(88.888948787062, 125.44615384615385, 'X[53] <= 0.031\\ngini = 0.216\\nsamples = 137\\nvalue = [0, 4, 121, 3, 0, 0, 0, 6, 3, 0]'),\n",
       " Text(83.02318059299192, 108.72, 'X[60] <= 0.438\\ngini = 0.375\\nsamples = 8\\nvalue = [0, 2, 0, 0, 0, 0, 0, 6, 0, 0]'),\n",
       " Text(81.21832884097036, 91.99384615384615, 'gini = 0.0\\nsamples = 6\\nvalue = [0, 0, 0, 0, 0, 0, 0, 6, 0, 0]'),\n",
       " Text(84.82803234501348, 91.99384615384615, 'gini = 0.0\\nsamples = 2\\nvalue = [0, 2, 0, 0, 0, 0, 0, 0, 0, 0]'),\n",
       " Text(94.75471698113208, 108.72, 'X[51] <= 0.406\\ngini = 0.119\\nsamples = 129\\nvalue = [0, 2, 121, 3, 0, 0, 0, 0, 3, 0]'),\n",
       " Text(88.43773584905661, 91.99384615384615, 'X[52] <= 0.781\\ngini = 0.694\\nsamples = 7\\nvalue = [0, 1, 2, 3, 0, 0, 0, 0, 1, 0]'),\n",
       " Text(86.63288409703505, 75.2676923076923, 'X[21] <= 0.656\\ngini = 0.625\\nsamples = 4\\nvalue = [0, 1, 2, 0, 0, 0, 0, 0, 1, 0]'),\n",
       " Text(84.82803234501348, 58.541538461538465, 'gini = 0.0\\nsamples = 2\\nvalue = [0, 0, 2, 0, 0, 0, 0, 0, 0, 0]'),\n",
       " Text(88.43773584905661, 58.541538461538465, 'X[11] <= 0.469\\ngini = 0.5\\nsamples = 2\\nvalue = [0, 1, 0, 0, 0, 0, 0, 0, 1, 0]'),\n",
       " Text(86.63288409703505, 41.81538461538463, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 1, 0, 0, 0, 0, 0, 0, 0, 0]'),\n",
       " Text(90.24258760107817, 41.81538461538463, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 0, 0, 0, 0, 0, 0, 0, 1, 0]'),\n",
       " Text(90.24258760107817, 75.2676923076923, 'gini = 0.0\\nsamples = 3\\nvalue = [0, 0, 0, 3, 0, 0, 0, 0, 0, 0]'),\n",
       " Text(101.07169811320756, 91.99384615384615, 'X[19] <= 0.875\\ngini = 0.048\\nsamples = 122\\nvalue = [0, 1, 119, 0, 0, 0, 0, 0, 2, 0]'),\n",
       " Text(99.26684636118598, 75.2676923076923, 'X[50] <= 0.031\\ngini = 0.033\\nsamples = 121\\nvalue = [0, 1, 119, 0, 0, 0, 0, 0, 1, 0]'),\n",
       " Text(95.65714285714286, 58.541538461538465, 'X[26] <= 0.125\\ngini = 0.5\\nsamples = 2\\nvalue = [0, 1, 1, 0, 0, 0, 0, 0, 0, 0]'),\n",
       " Text(93.8522911051213, 41.81538461538463, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 1, 0, 0, 0, 0, 0, 0, 0, 0]'),\n",
       " Text(97.46199460916442, 41.81538461538463, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]'),\n",
       " Text(102.87654986522912, 58.541538461538465, 'X[51] <= 0.594\\ngini = 0.017\\nsamples = 119\\nvalue = [0, 0, 118, 0, 0, 0, 0, 0, 1, 0]'),\n",
       " Text(101.07169811320756, 41.81538461538463, 'X[19] <= 0.438\\ngini = 0.375\\nsamples = 4\\nvalue = [0, 0, 3, 0, 0, 0, 0, 0, 1, 0]'),\n",
       " Text(99.26684636118598, 25.089230769230767, 'gini = 0.0\\nsamples = 3\\nvalue = [0, 0, 3, 0, 0, 0, 0, 0, 0, 0]'),\n",
       " Text(102.87654986522912, 25.089230769230767, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 0, 0, 0, 0, 0, 0, 0, 1, 0]'),\n",
       " Text(104.68140161725069, 41.81538461538463, 'gini = 0.0\\nsamples = 115\\nvalue = [0, 0, 115, 0, 0, 0, 0, 0, 0, 0]'),\n",
       " Text(102.87654986522912, 75.2676923076923, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 0, 0, 0, 0, 0, 0, 0, 1, 0]'),\n",
       " Text(117.54097035040432, 125.44615384615385, 'X[10] <= 0.406\\ngini = 0.715\\nsamples = 128\\nvalue = [0, 44, 20, 10, 0, 5, 1, 1, 47, 0]'),\n",
       " Text(108.29110512129381, 108.72, 'X[52] <= 0.375\\ngini = 0.307\\nsamples = 50\\nvalue = [0, 41, 0, 1, 0, 0, 1, 0, 7, 0]'),\n",
       " Text(106.48625336927225, 91.99384615384615, 'gini = 0.0\\nsamples = 7\\nvalue = [0, 0, 0, 0, 0, 0, 0, 0, 7, 0]'),\n",
       " Text(110.09595687331537, 91.99384615384615, 'X[7] <= 0.031\\ngini = 0.09\\nsamples = 43\\nvalue = [0, 41, 0, 1, 0, 0, 1, 0, 0, 0]'),\n",
       " Text(108.29110512129381, 75.2676923076923, 'X[45] <= 0.906\\ngini = 0.046\\nsamples = 42\\nvalue = [0, 41, 0, 0, 0, 0, 1, 0, 0, 0]'),\n",
       " Text(106.48625336927225, 58.541538461538465, 'gini = 0.0\\nsamples = 41\\nvalue = [0, 41, 0, 0, 0, 0, 0, 0, 0, 0]'),\n",
       " Text(110.09595687331537, 58.541538461538465, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 0, 0, 0, 0, 0, 1, 0, 0, 0]'),\n",
       " Text(111.90080862533694, 75.2676923076923, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]'),\n",
       " Text(126.79083557951483, 108.72, 'X[21] <= 0.281\\ngini = 0.652\\nsamples = 78\\nvalue = [0, 3, 20, 9, 0, 5, 0, 1, 40, 0]'),\n",
       " Text(119.12021563342319, 91.99384615384615, 'X[42] <= 0.156\\ngini = 0.497\\nsamples = 28\\nvalue = [0, 2, 19, 0, 0, 5, 0, 0, 2, 0]'),\n",
       " Text(115.51051212938006, 75.2676923076923, 'X[53] <= 0.406\\ngini = 0.531\\nsamples = 8\\nvalue = [0, 2, 0, 0, 0, 5, 0, 0, 1, 0]'),\n",
       " Text(113.7056603773585, 58.541538461538465, 'gini = 0.0\\nsamples = 5\\nvalue = [0, 0, 0, 0, 0, 5, 0, 0, 0, 0]'),\n",
       " Text(117.31536388140162, 58.541538461538465, 'X[28] <= 0.312\\ngini = 0.444\\nsamples = 3\\nvalue = [0, 2, 0, 0, 0, 0, 0, 0, 1, 0]'),\n",
       " Text(115.51051212938006, 41.81538461538463, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 0, 0, 0, 0, 0, 0, 0, 1, 0]'),\n",
       " Text(119.12021563342319, 41.81538461538463, 'gini = 0.0\\nsamples = 2\\nvalue = [0, 2, 0, 0, 0, 0, 0, 0, 0, 0]'),\n",
       " Text(122.72991913746631, 75.2676923076923, 'X[61] <= 0.188\\ngini = 0.095\\nsamples = 20\\nvalue = [0, 0, 19, 0, 0, 0, 0, 0, 1, 0]'),\n",
       " Text(120.92506738544475, 58.541538461538465, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 0, 0, 0, 0, 0, 0, 0, 1, 0]'),\n",
       " Text(124.53477088948787, 58.541538461538465, 'gini = 0.0\\nsamples = 19\\nvalue = [0, 0, 19, 0, 0, 0, 0, 0, 0, 0]'),\n",
       " Text(134.46145552560648, 91.99384615384615, 'X[18] <= 0.125\\ngini = 0.389\\nsamples = 50\\nvalue = [0, 1, 1, 9, 0, 0, 0, 1, 38, 0]'),\n",
       " Text(129.94932614555256, 75.2676923076923, 'X[29] <= 0.375\\ngini = 0.198\\nsamples = 9\\nvalue = [0, 1, 0, 8, 0, 0, 0, 0, 0, 0]'),\n",
       " Text(128.144474393531, 58.541538461538465, 'gini = 0.0\\nsamples = 8\\nvalue = [0, 0, 0, 8, 0, 0, 0, 0, 0, 0]'),\n",
       " Text(131.75417789757412, 58.541538461538465, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 1, 0, 0, 0, 0, 0, 0, 0, 0]'),\n",
       " Text(138.97358490566037, 75.2676923076923, 'X[43] <= 0.969\\ngini = 0.139\\nsamples = 41\\nvalue = [0, 0, 1, 1, 0, 0, 0, 1, 38, 0]'),\n",
       " Text(135.36388140161725, 58.541538461538465, 'X[42] <= 0.031\\ngini = 0.05\\nsamples = 39\\nvalue = [0, 0, 0, 1, 0, 0, 0, 0, 38, 0]'),\n",
       " Text(133.5590296495957, 41.81538461538463, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]'),\n",
       " Text(137.1687331536388, 41.81538461538463, 'gini = 0.0\\nsamples = 38\\nvalue = [0, 0, 0, 0, 0, 0, 0, 0, 38, 0]'),\n",
       " Text(142.5832884097035, 58.541538461538465, 'X[9] <= 0.094\\ngini = 0.5\\nsamples = 2\\nvalue = [0, 0, 1, 0, 0, 0, 0, 1, 0, 0]'),\n",
       " Text(140.77843665768194, 41.81538461538463, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 0, 0, 0, 0, 0, 0, 1, 0, 0]'),\n",
       " Text(144.38814016172506, 41.81538461538463, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]'),\n",
       " Text(145.29056603773586, 142.1723076923077, 'X[53] <= 0.406\\ngini = 0.281\\nsamples = 135\\nvalue = [0, 1, 1, 8, 3, 0, 4, 114, 3, 1]'),\n",
       " Text(141.68086253369273, 125.44615384615385, 'X[10] <= 0.031\\ngini = 0.034\\nsamples = 116\\nvalue = [0, 0, 0, 0, 1, 0, 1, 114, 0, 0]'),\n",
       " Text(139.87601078167117, 108.72, 'X[54] <= 0.5\\ngini = 0.5\\nsamples = 2\\nvalue = [0, 0, 0, 0, 1, 0, 1, 0, 0, 0]'),\n",
       " Text(138.0711590296496, 91.99384615384615, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 0, 0, 0, 1, 0, 0, 0, 0, 0]'),\n",
       " Text(141.68086253369273, 91.99384615384615, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 0, 0, 0, 0, 0, 1, 0, 0, 0]'),\n",
       " Text(143.4857142857143, 108.72, 'gini = 0.0\\nsamples = 114\\nvalue = [0, 0, 0, 0, 0, 0, 0, 114, 0, 0]'),\n",
       " Text(148.90026954177898, 125.44615384615385, 'X[58] <= 0.219\\ngini = 0.753\\nsamples = 19\\nvalue = [0, 1, 1, 8, 2, 0, 3, 0, 3, 1]'),\n",
       " Text(147.09541778975742, 108.72, 'X[5] <= 0.094\\ngini = 0.819\\nsamples = 12\\nvalue = [0, 1, 1, 1, 2, 0, 3, 0, 3, 1]'),\n",
       " Text(145.29056603773586, 91.99384615384615, 'gini = 0.0\\nsamples = 3\\nvalue = [0, 0, 0, 0, 0, 0, 3, 0, 0, 0]'),\n",
       " Text(148.90026954177898, 91.99384615384615, 'X[20] <= 0.188\\ngini = 0.79\\nsamples = 9\\nvalue = [0, 1, 1, 1, 2, 0, 0, 0, 3, 1]'),\n",
       " Text(147.09541778975742, 75.2676923076923, 'gini = 0.0\\nsamples = 3\\nvalue = [0, 0, 0, 0, 0, 0, 0, 0, 3, 0]'),\n",
       " Text(150.70512129380054, 75.2676923076923, 'X[3] <= 0.219\\ngini = 0.778\\nsamples = 6\\nvalue = [0, 1, 1, 1, 2, 0, 0, 0, 0, 1]'),\n",
       " Text(148.90026954177898, 58.541538461538465, 'gini = 0.0\\nsamples = 2\\nvalue = [0, 0, 0, 0, 2, 0, 0, 0, 0, 0]'),\n",
       " Text(152.5099730458221, 58.541538461538465, 'X[17] <= 0.219\\ngini = 0.75\\nsamples = 4\\nvalue = [0, 1, 1, 1, 0, 0, 0, 0, 0, 1]'),\n",
       " Text(148.90026954177898, 41.81538461538463, 'X[36] <= 0.75\\ngini = 0.5\\nsamples = 2\\nvalue = [0, 1, 1, 0, 0, 0, 0, 0, 0, 0]'),\n",
       " Text(147.09541778975742, 25.089230769230767, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]'),\n",
       " Text(150.70512129380054, 25.089230769230767, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 1, 0, 0, 0, 0, 0, 0, 0, 0]'),\n",
       " Text(156.11967654986523, 41.81538461538463, 'X[54] <= 0.719\\ngini = 0.5\\nsamples = 2\\nvalue = [0, 0, 0, 1, 0, 0, 0, 0, 0, 1]'),\n",
       " Text(154.31482479784367, 25.089230769230767, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]'),\n",
       " Text(157.9245283018868, 25.089230769230767, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]'),\n",
       " Text(150.70512129380054, 108.72, 'gini = 0.0\\nsamples = 7\\nvalue = [0, 0, 0, 7, 0, 0, 0, 0, 0, 0]'),\n",
       " Text(236.5201819407008, 175.62461538461537, 'X[21] <= 0.031\\ngini = 0.847\\nsamples = 777\\nvalue = [4, 103, 10, 9, 162, 132, 146, 37, 84, 90]'),\n",
       " Text(181.38760107816714, 158.89846153846153, 'X[42] <= 0.531\\ngini = 0.669\\nsamples = 336\\nvalue = [0, 24, 7, 5, 30, 123, 144, 0, 3, 0]'),\n",
       " Text(170.55849056603773, 142.1723076923077, 'X[5] <= 0.094\\ngini = 0.408\\nsamples = 158\\nvalue = [0, 16, 6, 5, 7, 120, 1, 0, 3, 0]'),\n",
       " Text(163.33908355795148, 125.44615384615385, 'X[19] <= 0.969\\ngini = 0.723\\nsamples = 35\\nvalue = [0, 16, 6, 4, 5, 1, 1, 0, 2, 0]'),\n",
       " Text(159.72938005390836, 108.72, 'X[62] <= 0.188\\ngini = 0.738\\nsamples = 15\\nvalue = [0, 0, 6, 3, 3, 1, 0, 0, 2, 0]'),\n",
       " Text(157.9245283018868, 91.99384615384615, 'X[9] <= 0.031\\ngini = 0.716\\nsamples = 9\\nvalue = [0, 0, 0, 3, 3, 1, 0, 0, 2, 0]'),\n",
       " Text(156.11967654986523, 75.2676923076923, 'gini = 0.0\\nsamples = 3\\nvalue = [0, 0, 0, 0, 3, 0, 0, 0, 0, 0]'),\n",
       " Text(159.72938005390836, 75.2676923076923, 'X[17] <= 0.281\\ngini = 0.611\\nsamples = 6\\nvalue = [0, 0, 0, 3, 0, 1, 0, 0, 2, 0]'),\n",
       " Text(157.9245283018868, 58.541538461538465, 'gini = 0.0\\nsamples = 3\\nvalue = [0, 0, 0, 3, 0, 0, 0, 0, 0, 0]'),\n",
       " Text(161.53423180592992, 58.541538461538465, 'X[20] <= 0.312\\ngini = 0.444\\nsamples = 3\\nvalue = [0, 0, 0, 0, 0, 1, 0, 0, 2, 0]'),\n",
       " Text(159.72938005390836, 41.81538461538463, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]'),\n",
       " Text(163.33908355795148, 41.81538461538463, 'gini = 0.0\\nsamples = 2\\nvalue = [0, 0, 0, 0, 0, 0, 0, 0, 2, 0]'),\n",
       " Text(161.53423180592992, 91.99384615384615, 'gini = 0.0\\nsamples = 6\\nvalue = [0, 0, 6, 0, 0, 0, 0, 0, 0, 0]'),\n",
       " Text(166.9487870619946, 108.72, 'X[4] <= 0.719\\ngini = 0.345\\nsamples = 20\\nvalue = [0, 16, 0, 1, 2, 0, 1, 0, 0, 0]'),\n",
       " Text(165.14393530997305, 91.99384615384615, 'gini = 0.0\\nsamples = 16\\nvalue = [0, 16, 0, 0, 0, 0, 0, 0, 0, 0]'),\n",
       " Text(168.75363881401617, 91.99384615384615, 'X[27] <= 0.969\\ngini = 0.625\\nsamples = 4\\nvalue = [0, 0, 0, 1, 2, 0, 1, 0, 0, 0]'),\n",
       " Text(166.9487870619946, 75.2676923076923, 'gini = 0.0\\nsamples = 2\\nvalue = [0, 0, 0, 0, 2, 0, 0, 0, 0, 0]'),\n",
       " Text(170.55849056603773, 75.2676923076923, 'X[5] <= 0.031\\ngini = 0.5\\nsamples = 2\\nvalue = [0, 0, 0, 1, 0, 0, 1, 0, 0, 0]'),\n",
       " Text(168.75363881401617, 58.541538461538465, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 0, 0, 0, 0, 0, 1, 0, 0, 0]'),\n",
       " Text(172.3633423180593, 58.541538461538465, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]'),\n",
       " Text(177.777897574124, 125.44615384615385, 'X[18] <= 0.281\\ngini = 0.064\\nsamples = 123\\nvalue = [0, 0, 0, 1, 2, 119, 0, 0, 1, 0]'),\n",
       " Text(174.1681940700809, 108.72, 'X[28] <= 0.406\\ngini = 0.5\\nsamples = 2\\nvalue = [0, 0, 0, 1, 1, 0, 0, 0, 0, 0]'),\n",
       " Text(172.3633423180593, 91.99384615384615, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]'),\n",
       " Text(175.97304582210245, 91.99384615384615, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 0, 0, 0, 1, 0, 0, 0, 0, 0]'),\n",
       " Text(181.38760107816714, 108.72, 'X[8] <= 0.094\\ngini = 0.033\\nsamples = 121\\nvalue = [0, 0, 0, 0, 1, 119, 0, 0, 1, 0]'),\n",
       " Text(179.58274932614557, 91.99384615384615, 'X[24] <= 0.031\\ngini = 0.017\\nsamples = 120\\nvalue = [0, 0, 0, 0, 1, 119, 0, 0, 0, 0]'),\n",
       " Text(177.777897574124, 75.2676923076923, 'gini = 0.0\\nsamples = 119\\nvalue = [0, 0, 0, 0, 0, 119, 0, 0, 0, 0]'),\n",
       " Text(181.38760107816714, 75.2676923076923, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 0, 0, 0, 1, 0, 0, 0, 0, 0]'),\n",
       " Text(183.1924528301887, 91.99384615384615, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 0, 0, 0, 0, 0, 0, 0, 1, 0]'),\n",
       " Text(192.2167115902965, 142.1723076923077, 'X[54] <= 0.094\\ngini = 0.336\\nsamples = 178\\nvalue = [0, 8, 1, 0, 23, 3, 143, 0, 0, 0]'),\n",
       " Text(188.6070080862534, 125.44615384615385, 'X[45] <= 0.094\\ngini = 0.613\\nsamples = 41\\nvalue = [0, 8, 0, 0, 23, 3, 7, 0, 0, 0]'),\n",
       " Text(186.80215633423182, 108.72, 'gini = 0.0\\nsamples = 8\\nvalue = [0, 8, 0, 0, 0, 0, 0, 0, 0, 0]'),\n",
       " Text(190.41185983827495, 108.72, 'X[10] <= 0.344\\ngini = 0.461\\nsamples = 33\\nvalue = [0, 0, 0, 0, 23, 3, 7, 0, 0, 0]'),\n",
       " Text(186.80215633423182, 91.99384615384615, 'X[42] <= 0.719\\ngini = 0.087\\nsamples = 22\\nvalue = [0, 0, 0, 0, 21, 0, 1, 0, 0, 0]'),\n",
       " Text(184.99730458221026, 75.2676923076923, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 0, 0, 0, 0, 0, 1, 0, 0, 0]'),\n",
       " Text(188.6070080862534, 75.2676923076923, 'gini = 0.0\\nsamples = 21\\nvalue = [0, 0, 0, 0, 21, 0, 0, 0, 0, 0]'),\n",
       " Text(194.02156334231807, 91.99384615384615, 'X[50] <= 0.375\\ngini = 0.595\\nsamples = 11\\nvalue = [0, 0, 0, 0, 2, 3, 6, 0, 0, 0]'),\n",
       " Text(192.2167115902965, 75.2676923076923, 'X[35] <= 0.625\\ngini = 0.48\\nsamples = 5\\nvalue = [0, 0, 0, 0, 2, 3, 0, 0, 0, 0]'),\n",
       " Text(190.41185983827495, 58.541538461538465, 'gini = 0.0\\nsamples = 2\\nvalue = [0, 0, 0, 0, 2, 0, 0, 0, 0, 0]'),\n",
       " Text(194.02156334231807, 58.541538461538465, 'gini = 0.0\\nsamples = 3\\nvalue = [0, 0, 0, 0, 0, 3, 0, 0, 0, 0]'),\n",
       " Text(195.82641509433964, 75.2676923076923, 'gini = 0.0\\nsamples = 6\\nvalue = [0, 0, 0, 0, 0, 0, 6, 0, 0, 0]'),\n",
       " Text(195.82641509433964, 125.44615384615385, 'X[10] <= 0.969\\ngini = 0.014\\nsamples = 137\\nvalue = [0, 0, 1, 0, 0, 0, 136, 0, 0, 0]'),\n",
       " Text(194.02156334231807, 108.72, 'gini = 0.0\\nsamples = 136\\nvalue = [0, 0, 0, 0, 0, 0, 136, 0, 0, 0]'),\n",
       " Text(197.6312668463612, 108.72, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]'),\n",
       " Text(291.6527628032345, 158.89846153846153, 'X[33] <= 0.281\\ngini = 0.795\\nsamples = 441\\nvalue = [4, 79, 3, 4, 132, 9, 2, 37, 81, 90]'),\n",
       " Text(267.9076819407008, 142.1723076923077, 'X[20] <= 0.969\\ngini = 0.782\\nsamples = 304\\nvalue = [3, 67, 3, 4, 18, 9, 0, 32, 79, 89]'),\n",
       " Text(247.0390835579515, 125.44615384615385, 'X[42] <= 0.281\\ngini = 0.721\\nsamples = 222\\nvalue = [3, 7, 1, 2, 14, 9, 0, 27, 75, 84]'),\n",
       " Text(227.8625336927224, 108.72, 'X[43] <= 0.219\\ngini = 0.653\\nsamples = 152\\nvalue = [0, 4, 1, 2, 10, 9, 0, 23, 20, 83]'),\n",
       " Text(209.81401617250674, 91.99384615384615, 'X[21] <= 0.438\\ngini = 0.401\\nsamples = 98\\nvalue = [0, 4, 0, 2, 2, 8, 0, 6, 1, 75]'),\n",
       " Text(201.24097035040433, 75.2676923076923, 'X[2] <= 0.219\\ngini = 0.639\\nsamples = 13\\nvalue = [0, 0, 0, 1, 1, 7, 0, 3, 0, 1]'),\n",
       " Text(197.6312668463612, 58.541538461538465, 'X[38] <= 0.062\\ngini = 0.56\\nsamples = 5\\nvalue = [0, 0, 0, 0, 1, 0, 0, 3, 0, 1]'),\n",
       " Text(195.82641509433964, 41.81538461538463, 'gini = 0.0\\nsamples = 3\\nvalue = [0, 0, 0, 0, 0, 0, 0, 3, 0, 0]'),\n",
       " Text(199.43611859838276, 41.81538461538463, 'X[14] <= 0.062\\ngini = 0.5\\nsamples = 2\\nvalue = [0, 0, 0, 0, 1, 0, 0, 0, 0, 1]'),\n",
       " Text(197.6312668463612, 25.089230769230767, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]'),\n",
       " Text(201.24097035040433, 25.089230769230767, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 0, 0, 0, 1, 0, 0, 0, 0, 0]'),\n",
       " Text(204.85067385444745, 58.541538461538465, 'X[18] <= 0.594\\ngini = 0.219\\nsamples = 8\\nvalue = [0, 0, 0, 1, 0, 7, 0, 0, 0, 0]'),\n",
       " Text(203.0458221024259, 41.81538461538463, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]'),\n",
       " Text(206.655525606469, 41.81538461538463, 'gini = 0.0\\nsamples = 7\\nvalue = [0, 0, 0, 0, 0, 7, 0, 0, 0, 0]'),\n",
       " Text(218.38706199460918, 75.2676923076923, 'X[27] <= 0.219\\ngini = 0.238\\nsamples = 85\\nvalue = [0, 4, 0, 1, 1, 1, 0, 3, 1, 74]'),\n",
       " Text(212.0700808625337, 58.541538461538465, 'X[59] <= 0.281\\ngini = 0.7\\nsamples = 10\\nvalue = [0, 4, 0, 1, 0, 0, 0, 2, 0, 3]'),\n",
       " Text(210.26522911051214, 41.81538461538463, 'gini = 0.0\\nsamples = 4\\nvalue = [0, 4, 0, 0, 0, 0, 0, 0, 0, 0]'),\n",
       " Text(213.87493261455526, 41.81538461538463, 'X[58] <= 0.125\\ngini = 0.611\\nsamples = 6\\nvalue = [0, 0, 0, 1, 0, 0, 0, 2, 0, 3]'),\n",
       " Text(212.0700808625337, 25.089230769230767, 'X[29] <= 0.531\\ngini = 0.444\\nsamples = 3\\nvalue = [0, 0, 0, 1, 0, 0, 0, 2, 0, 0]'),\n",
       " Text(210.26522911051214, 8.363076923076932, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]'),\n",
       " Text(213.87493261455526, 8.363076923076932, 'gini = 0.0\\nsamples = 2\\nvalue = [0, 0, 0, 0, 0, 0, 0, 2, 0, 0]'),\n",
       " Text(215.67978436657683, 25.089230769230767, 'gini = 0.0\\nsamples = 3\\nvalue = [0, 0, 0, 0, 0, 0, 0, 0, 0, 3]'),\n",
       " Text(224.70404312668464, 58.541538461538465, 'X[33] <= 0.188\\ngini = 0.103\\nsamples = 75\\nvalue = [0, 0, 0, 0, 1, 1, 0, 1, 1, 71]'),\n",
       " Text(221.0943396226415, 41.81538461538463, 'X[4] <= 0.031\\ngini = 0.054\\nsamples = 73\\nvalue = [0, 0, 0, 0, 0, 1, 0, 0, 1, 71]'),\n",
       " Text(219.28948787061995, 25.089230769230767, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]'),\n",
       " Text(222.89919137466308, 25.089230769230767, 'X[28] <= 0.25\\ngini = 0.027\\nsamples = 72\\nvalue = [0, 0, 0, 0, 0, 0, 0, 0, 1, 71]'),\n",
       " Text(221.0943396226415, 8.363076923076932, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 0, 0, 0, 0, 0, 0, 0, 1, 0]'),\n",
       " Text(224.70404312668464, 8.363076923076932, 'gini = 0.0\\nsamples = 71\\nvalue = [0, 0, 0, 0, 0, 0, 0, 0, 0, 71]'),\n",
       " Text(228.31374663072776, 41.81538461538463, 'X[51] <= 0.438\\ngini = 0.5\\nsamples = 2\\nvalue = [0, 0, 0, 0, 1, 0, 0, 1, 0, 0]'),\n",
       " Text(226.5088948787062, 25.089230769230767, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 0, 0, 0, 1, 0, 0, 0, 0, 0]'),\n",
       " Text(230.11859838274933, 25.089230769230767, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 0, 0, 0, 0, 0, 0, 1, 0, 0]'),\n",
       " Text(245.91105121293802, 91.99384615384615, 'X[53] <= 0.375\\ngini = 0.733\\nsamples = 54\\nvalue = [0, 0, 1, 0, 8, 1, 0, 17, 19, 8]'),\n",
       " Text(238.24043126684637, 75.2676923076923, 'X[5] <= 0.062\\ngini = 0.641\\nsamples = 33\\nvalue = [0, 0, 0, 0, 8, 1, 0, 17, 1, 6]'),\n",
       " Text(233.72830188679248, 58.541538461538465, 'X[58] <= 0.844\\ngini = 0.219\\nsamples = 8\\nvalue = [0, 0, 0, 0, 7, 1, 0, 0, 0, 0]'),\n",
       " Text(231.92345013477092, 41.81538461538463, 'gini = 0.0\\nsamples = 7\\nvalue = [0, 0, 0, 0, 7, 0, 0, 0, 0, 0]'),\n",
       " Text(235.53315363881404, 41.81538461538463, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]'),\n",
       " Text(242.7525606469003, 58.541538461538465, 'X[17] <= 0.25\\ngini = 0.477\\nsamples = 25\\nvalue = [0, 0, 0, 0, 1, 0, 0, 17, 1, 6]'),\n",
       " Text(239.14285714285717, 41.81538461538463, 'X[2] <= 0.094\\ngini = 0.105\\nsamples = 18\\nvalue = [0, 0, 0, 0, 1, 0, 0, 17, 0, 0]'),\n",
       " Text(237.3380053908356, 25.089230769230767, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 0, 0, 0, 1, 0, 0, 0, 0, 0]'),\n",
       " Text(240.94770889487873, 25.089230769230767, 'gini = 0.0\\nsamples = 17\\nvalue = [0, 0, 0, 0, 0, 0, 0, 17, 0, 0]'),\n",
       " Text(246.36226415094342, 41.81538461538463, 'X[18] <= 0.562\\ngini = 0.245\\nsamples = 7\\nvalue = [0, 0, 0, 0, 0, 0, 0, 0, 1, 6]'),\n",
       " Text(244.55741239892185, 25.089230769230767, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 0, 0, 0, 0, 0, 0, 0, 1, 0]'),\n",
       " Text(248.16711590296498, 25.089230769230767, 'gini = 0.0\\nsamples = 6\\nvalue = [0, 0, 0, 0, 0, 0, 0, 0, 0, 6]'),\n",
       " Text(253.58167115902967, 75.2676923076923, 'X[58] <= 0.406\\ngini = 0.254\\nsamples = 21\\nvalue = [0, 0, 1, 0, 0, 0, 0, 0, 18, 2]'),\n",
       " Text(251.7768194070081, 58.541538461538465, 'X[17] <= 0.719\\ngini = 0.1\\nsamples = 19\\nvalue = [0, 0, 1, 0, 0, 0, 0, 0, 18, 0]'),\n",
       " Text(249.97196765498654, 41.81538461538463, 'gini = 0.0\\nsamples = 18\\nvalue = [0, 0, 0, 0, 0, 0, 0, 0, 18, 0]'),\n",
       " Text(253.58167115902967, 41.81538461538463, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]'),\n",
       " Text(255.38652291105123, 58.541538461538465, 'gini = 0.0\\nsamples = 2\\nvalue = [0, 0, 0, 0, 0, 0, 0, 0, 0, 2]'),\n",
       " Text(266.21563342318063, 108.72, 'X[38] <= 0.062\\ngini = 0.372\\nsamples = 70\\nvalue = [3, 3, 0, 0, 4, 0, 0, 4, 55, 1]'),\n",
       " Text(262.6059299191375, 91.99384615384615, 'X[19] <= 0.969\\ngini = 0.159\\nsamples = 59\\nvalue = [1, 3, 0, 0, 1, 0, 0, 0, 54, 0]'),\n",
       " Text(260.8010781671159, 75.2676923076923, 'X[31] <= 0.031\\ngini = 0.07\\nsamples = 56\\nvalue = [1, 0, 0, 0, 1, 0, 0, 0, 54, 0]'),\n",
       " Text(258.9962264150943, 58.541538461538465, 'X[37] <= 0.844\\ngini = 0.036\\nsamples = 55\\nvalue = [1, 0, 0, 0, 0, 0, 0, 0, 54, 0]'),\n",
       " Text(257.1913746630728, 41.81538461538463, 'gini = 0.0\\nsamples = 54\\nvalue = [0, 0, 0, 0, 0, 0, 0, 0, 54, 0]'),\n",
       " Text(260.8010781671159, 41.81538461538463, 'gini = 0.0\\nsamples = 1\\nvalue = [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]'),\n",
       " Text(262.6059299191375, 58.541538461538465, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 0, 0, 0, 1, 0, 0, 0, 0, 0]'),\n",
       " Text(264.41078167115904, 75.2676923076923, 'gini = 0.0\\nsamples = 3\\nvalue = [0, 3, 0, 0, 0, 0, 0, 0, 0, 0]'),\n",
       " Text(269.82533692722376, 91.99384615384615, 'X[60] <= 0.469\\ngini = 0.744\\nsamples = 11\\nvalue = [2, 0, 0, 0, 3, 0, 0, 4, 1, 1]'),\n",
       " Text(268.02048517520217, 75.2676923076923, 'gini = 0.0\\nsamples = 4\\nvalue = [0, 0, 0, 0, 0, 0, 0, 4, 0, 0]'),\n",
       " Text(271.6301886792453, 75.2676923076923, 'X[51] <= 0.5\\ngini = 0.694\\nsamples = 7\\nvalue = [2, 0, 0, 0, 3, 0, 0, 0, 1, 1]'),\n",
       " Text(269.82533692722376, 58.541538461538465, 'gini = 0.0\\nsamples = 3\\nvalue = [0, 0, 0, 0, 3, 0, 0, 0, 0, 0]'),\n",
       " Text(273.4350404312669, 58.541538461538465, 'X[54] <= 0.219\\ngini = 0.625\\nsamples = 4\\nvalue = [2, 0, 0, 0, 0, 0, 0, 0, 1, 1]'),\n",
       " Text(271.6301886792453, 41.81538461538463, 'gini = 0.0\\nsamples = 2\\nvalue = [2, 0, 0, 0, 0, 0, 0, 0, 0, 0]'),\n",
       " Text(275.2398921832884, 41.81538461538463, 'X[20] <= 0.438\\ngini = 0.5\\nsamples = 2\\nvalue = [0, 0, 0, 0, 0, 0, 0, 0, 1, 1]'),\n",
       " Text(273.4350404312669, 25.089230769230767, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 0, 0, 0, 0, 0, 0, 0, 1, 0]'),\n",
       " Text(277.04474393531, 25.089230769230767, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]'),\n",
       " Text(288.7762803234501, 125.44615384615385, 'X[9] <= 0.031\\ngini = 0.451\\nsamples = 82\\nvalue = [0, 60, 2, 2, 4, 0, 0, 5, 4, 5]'),\n",
       " Text(284.26415094339626, 108.72, 'X[41] <= 0.375\\ngini = 0.22\\nsamples = 67\\nvalue = [0, 59, 0, 0, 4, 0, 0, 2, 1, 1]'),\n",
       " Text(282.45929919137467, 91.99384615384615, 'X[52] <= 0.188\\ngini = 0.148\\nsamples = 64\\nvalue = [0, 59, 0, 0, 1, 0, 0, 2, 1, 1]'),\n",
       " Text(278.84959568733154, 75.2676923076923, 'X[41] <= 0.156\\ngini = 0.444\\nsamples = 3\\nvalue = [0, 0, 0, 0, 0, 0, 0, 2, 0, 1]'),\n",
       " Text(277.04474393531, 58.541538461538465, 'gini = 0.0\\nsamples = 2\\nvalue = [0, 0, 0, 0, 0, 0, 0, 2, 0, 0]'),\n",
       " Text(280.65444743935313, 58.541538461538465, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]'),\n",
       " Text(286.0690026954178, 75.2676923076923, 'X[13] <= 0.094\\ngini = 0.064\\nsamples = 61\\nvalue = [0, 59, 0, 0, 1, 0, 0, 0, 1, 0]'),\n",
       " Text(284.26415094339626, 58.541538461538465, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 0, 0, 0, 1, 0, 0, 0, 0, 0]'),\n",
       " Text(287.8738544474394, 58.541538461538465, 'X[28] <= 0.438\\ngini = 0.033\\nsamples = 60\\nvalue = [0, 59, 0, 0, 0, 0, 0, 0, 1, 0]'),\n",
       " Text(286.0690026954178, 41.81538461538463, 'X[38] <= 0.125\\ngini = 0.5\\nsamples = 2\\nvalue = [0, 1, 0, 0, 0, 0, 0, 0, 1, 0]'),\n",
       " Text(284.26415094339626, 25.089230769230767, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 0, 0, 0, 0, 0, 0, 0, 1, 0]'),\n",
       " Text(287.8738544474394, 25.089230769230767, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 1, 0, 0, 0, 0, 0, 0, 0, 0]'),\n",
       " Text(289.6787061994609, 41.81538461538463, 'gini = 0.0\\nsamples = 58\\nvalue = [0, 58, 0, 0, 0, 0, 0, 0, 0, 0]'),\n",
       " Text(286.0690026954178, 91.99384615384615, 'gini = 0.0\\nsamples = 3\\nvalue = [0, 0, 0, 0, 3, 0, 0, 0, 0, 0]'),\n",
       " Text(293.28840970350404, 108.72, 'X[52] <= 0.062\\ngini = 0.809\\nsamples = 15\\nvalue = [0, 1, 2, 2, 0, 0, 0, 3, 3, 4]'),\n",
       " Text(291.4835579514825, 91.99384615384615, 'gini = 0.0\\nsamples = 3\\nvalue = [0, 0, 0, 0, 0, 0, 0, 3, 0, 0]'),\n",
       " Text(295.09326145552564, 91.99384615384615, 'X[29] <= 0.688\\ngini = 0.764\\nsamples = 12\\nvalue = [0, 1, 2, 2, 0, 0, 0, 0, 3, 4]'),\n",
       " Text(293.28840970350404, 75.2676923076923, 'X[62] <= 0.031\\ngini = 0.719\\nsamples = 8\\nvalue = [0, 1, 2, 2, 0, 0, 0, 0, 3, 0]'),\n",
       " Text(291.4835579514825, 58.541538461538465, 'gini = 0.0\\nsamples = 3\\nvalue = [0, 0, 0, 0, 0, 0, 0, 0, 3, 0]'),\n",
       " Text(295.09326145552564, 58.541538461538465, 'X[45] <= 0.219\\ngini = 0.64\\nsamples = 5\\nvalue = [0, 1, 2, 2, 0, 0, 0, 0, 0, 0]'),\n",
       " Text(293.28840970350404, 41.81538461538463, 'gini = 0.0\\nsamples = 2\\nvalue = [0, 0, 2, 0, 0, 0, 0, 0, 0, 0]'),\n",
       " Text(296.89811320754717, 41.81538461538463, 'X[61] <= 0.656\\ngini = 0.444\\nsamples = 3\\nvalue = [0, 1, 0, 2, 0, 0, 0, 0, 0, 0]'),\n",
       " Text(295.09326145552564, 25.089230769230767, 'gini = 0.0\\nsamples = 2\\nvalue = [0, 0, 0, 2, 0, 0, 0, 0, 0, 0]'),\n",
       " Text(298.70296495956876, 25.089230769230767, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 1, 0, 0, 0, 0, 0, 0, 0, 0]'),\n",
       " Text(296.89811320754717, 75.2676923076923, 'gini = 0.0\\nsamples = 4\\nvalue = [0, 0, 0, 0, 0, 0, 0, 0, 0, 4]'),\n",
       " Text(315.3978436657682, 142.1723076923077, 'X[27] <= 0.906\\ngini = 0.298\\nsamples = 137\\nvalue = [1, 12, 0, 0, 114, 0, 2, 5, 2, 1]'),\n",
       " Text(307.72722371967654, 125.44615384615385, 'X[5] <= 0.719\\ngini = 0.131\\nsamples = 117\\nvalue = [1, 0, 0, 0, 109, 0, 1, 4, 1, 1]'),\n",
       " Text(304.1175202156334, 108.72, 'X[13] <= 0.969\\ngini = 0.036\\nsamples = 110\\nvalue = [1, 0, 0, 0, 108, 0, 0, 0, 1, 0]'),\n",
       " Text(302.3126684636119, 91.99384615384615, 'X[36] <= 0.125\\ngini = 0.018\\nsamples = 109\\nvalue = [1, 0, 0, 0, 108, 0, 0, 0, 0, 0]'),\n",
       " Text(300.5078167115903, 75.2676923076923, 'gini = 0.0\\nsamples = 1\\nvalue = [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]'),\n",
       " Text(304.1175202156334, 75.2676923076923, 'gini = 0.0\\nsamples = 108\\nvalue = [0, 0, 0, 0, 108, 0, 0, 0, 0, 0]'),\n",
       " Text(305.922371967655, 91.99384615384615, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 0, 0, 0, 0, 0, 0, 0, 1, 0]'),\n",
       " Text(311.33692722371967, 108.72, 'X[19] <= 0.344\\ngini = 0.612\\nsamples = 7\\nvalue = [0, 0, 0, 0, 1, 0, 1, 4, 0, 1]'),\n",
       " Text(309.53207547169814, 91.99384615384615, 'gini = 0.0\\nsamples = 4\\nvalue = [0, 0, 0, 0, 0, 0, 0, 4, 0, 0]'),\n",
       " Text(313.14177897574126, 91.99384615384615, 'X[17] <= 0.062\\ngini = 0.667\\nsamples = 3\\nvalue = [0, 0, 0, 0, 1, 0, 1, 0, 0, 1]'),\n",
       " Text(311.33692722371967, 75.2676923076923, 'X[61] <= 0.562\\ngini = 0.5\\nsamples = 2\\nvalue = [0, 0, 0, 0, 1, 0, 0, 0, 0, 1]'),\n",
       " Text(309.53207547169814, 58.541538461538465, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 0, 0, 0, 1, 0, 0, 0, 0, 0]'),\n",
       " Text(313.14177897574126, 58.541538461538465, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]'),\n",
       " Text(314.9466307277628, 75.2676923076923, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 0, 0, 0, 0, 0, 1, 0, 0, 0]'),\n",
       " Text(323.06846361185984, 125.44615384615385, 'X[38] <= 0.062\\ngini = 0.57\\nsamples = 20\\nvalue = [0, 12, 0, 0, 5, 0, 1, 1, 1, 0]'),\n",
       " Text(318.556334231806, 108.72, 'X[9] <= 0.219\\ngini = 0.165\\nsamples = 11\\nvalue = [0, 10, 0, 0, 0, 0, 0, 0, 1, 0]'),\n",
       " Text(316.7514824797844, 91.99384615384615, 'gini = 0.0\\nsamples = 10\\nvalue = [0, 10, 0, 0, 0, 0, 0, 0, 0, 0]'),\n",
       " Text(320.3611859838275, 91.99384615384615, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 0, 0, 0, 0, 0, 0, 0, 1, 0]'),\n",
       " Text(327.58059299191376, 108.72, 'X[45] <= 0.812\\ngini = 0.617\\nsamples = 9\\nvalue = [0, 2, 0, 0, 5, 0, 1, 1, 0, 0]'),\n",
       " Text(323.97088948787064, 91.99384615384615, 'X[30] <= 0.75\\ngini = 0.278\\nsamples = 6\\nvalue = [0, 0, 0, 0, 5, 0, 0, 1, 0, 0]'),\n",
       " Text(322.1660377358491, 75.2676923076923, 'gini = 0.0\\nsamples = 5\\nvalue = [0, 0, 0, 0, 5, 0, 0, 0, 0, 0]'),\n",
       " Text(325.7757412398922, 75.2676923076923, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 0, 0, 0, 0, 0, 0, 1, 0, 0]'),\n",
       " Text(331.1902964959569, 91.99384615384615, 'X[19] <= 0.812\\ngini = 0.444\\nsamples = 3\\nvalue = [0, 2, 0, 0, 0, 0, 1, 0, 0, 0]'),\n",
       " Text(329.38544474393535, 75.2676923076923, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 0, 0, 0, 0, 0, 1, 0, 0, 0]'),\n",
       " Text(332.9951482479785, 75.2676923076923, 'gini = 0.0\\nsamples = 2\\nvalue = [0, 2, 0, 0, 0, 0, 0, 0, 0, 0]')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sklearn.tree.plot_tree(tree_model)\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": [
     "\n",
     "import sys\n",
     "\n",
     "import sklearn\n",
     "from sklearn.datasets import load_digits\n",
     "\n",
     "sys.path.append('..')\n",
     "import torch\n",
     "import numpy as np\n",
     "import matplotlib.pyplot as plt\n",
     "\n",
     "from deep_logic.utils.relunn import get_reduced_model, prune_features\n",
     "from deep_logic import fol\n",
     "import deep_logic as dl\n",
     "\n",
     "torch.manual_seed(0)\n",
     "np.random.seed(0)\n",
     "\n",
     "# %%\n",
     "\n",
     "X, y = load_digits(return_X_y=True)\n",
     "y = sklearn.preprocessing.OneHotEncoder().fit_transform(y)\n",
     "X -= X.min()\n",
     "X /= X.max()\n",
     "print(f'X shape: {X.shape}\\nClasses: {np.unique(y)}')\n",
     "print(f'X max: {X.max()}')\n",
     "\n",
     "# %%\n",
     "\n",
     "x_train = torch.tensor(X, dtype=torch.float)\n",
     "print(x_train.shape)\n",
     "\n",
     "# %%\n",
     "\n",
     "y_train = torch.zeros((y.shape[0], y.shape[1]), dtype=torch.float)\n",
     "y_train = torch.tensor(y, dtype=torch.float)\n",
     "x_test = x_train\n",
     "n_classes = y_train.size(1)\n",
     "print(n_classes)\n",
     "y_train\n",
     "\n",
     "# %%\n",
     "\n",
     "y_train.sum(dim=0)\n",
     "\n",
     "# %%\n",
     "\n",
     "torch.manual_seed(0)\n",
     "np.random.seed(0)\n",
     "\n",
     "layers = [\n",
     "    torch.nn.Linear(x_train.size(1), 20 * n_classes),\n",
     "    torch.nn.LeakyReLU(),\n",
     "    dl.nn.XLinear(20, 10, n_classes),\n",
     "    torch.nn.LeakyReLU(),\n",
     "    dl.nn.XLinear(10, 5, n_classes),\n",
     "    torch.nn.LeakyReLU(),\n",
     "    dl.nn.XLinear(5, 1, n_classes),\n",
     "    torch.nn.Softmax(),\n",
     "]\n",
     "model = torch.nn.Sequential(*layers)\n",
     "\n",
     "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
     "loss_form = torch.nn.BCELoss()\n",
     "model.train()\n",
     "need_pruning = True\n",
     "for epoch in range(2000):\n",
     "    # forward pass\n",
     "    optimizer.zero_grad()\n",
     "    y_pred = model(x_train)\n",
     "    # Compute Loss\n",
     "    loss = loss_form(y_pred, y_train)\n",
     "\n",
     "    for module in model.children():\n",
     "        if isinstance(module, torch.nn.Linear):\n",
     "            loss += 0.001 * torch.norm(module.weight, 1)\n",
     "            break\n",
     "\n",
     "    # backward pass\n",
     "    loss.backward()\n",
     "    optimizer.step()\n",
     "\n",
     "    if epoch > 1000 and need_pruning:\n",
     "        prune_features(model, n_classes)\n",
     "        # need_pruning = False\n",
     "\n",
     "    # compute accuracy\n",
     "    if epoch % 500 == 0:\n",
     "        y_pred_d = torch.argmax(y_pred, dim=1)\n",
     "        y_train_d = torch.argmax(y_train, dim=1)\n",
     "        accuracy = y_pred_d.eq(y_train_d).sum().item() / y_train.size(0)\n",
     "        print(f'Epoch {epoch}: train accuracy: {accuracy:.4f}')\n",
     "\n",
     "# %% md\n",
     "\n",
     "# Local explanations\n",
     "\n",
     "# %%\n",
     "\n",
     "np.set_printoptions(precision=2, suppress=True)\n",
     "outputs = []\n",
     "for i, (xin, yin) in enumerate(zip(x_train, y_train)):\n",
     "    model_reduced = get_reduced_model(model, xin)\n",
     "    for module in model_reduced.children():\n",
     "        if isinstance(module, torch.nn.Linear):\n",
     "            wa = module.weight.detach().numpy()\n",
     "            ba = module.bias.detach().item()\n",
     "            break\n",
     "    output = model_reduced(xin)\n",
     "\n",
     "    pred_class = torch.argmax(output)\n",
     "    true_class = torch.argmax(y_train[i])\n",
     "\n",
     "    # generate local explanation only if the prediction is correct\n",
     "    if pred_class.eq(true_class):\n",
     "        local_explanation = fol.relunn.explain_local(model, x_train, y_train, xin)\n",
     "        print(f'Input {(i + 1)}')\n",
     "        print(f'\\tx={xin.detach().numpy()}')\n",
     "        print(f'\\ty={output.detach().numpy()}, y_label={yin}')\n",
     "        print(f'\\tw={wa}')\n",
     "        print(f'\\tb={ba}')\n",
     "        print(f'\\tExplanation: {local_explanation}')\n",
     "        print()\n",
     "        xin = xin.reshape(8, 8)\n",
     "        plt.figure(1, figsize=(3, 3))\n",
     "        plt.imshow(xin, cmap=plt.cm.gray_r, interpolation='nearest')\n",
     "        plt.show()\n",
     "        wa = wa.reshape(8, 8)\n",
     "        plt.figure(1, figsize=(3, 3))\n",
     "        plt.imshow(wa * xin.numpy(), cmap=plt.cm.gray_r, interpolation='nearest')\n",
     "        plt.show()\n",
     "\n",
     "    outputs.append(output)\n",
     "    if i > 10:\n",
     "        break\n",
     "\n",
     "# %% md\n",
     "\n",
     "# Combine local explanations\n",
     "\n",
     "# %%\n",
     "\n",
     "y_train_d = torch.argmax(y_train, dim=1)\n",
     "for target_class in range(n_classes):\n",
     "    global_explanation, predictions, counter = fol.combine_local_explanations(model, x_train, y_train,\n",
     "                                                                              topk_explanations=10,\n",
     "                                                                              target_class=target_class)\n",
     "\n",
     "    y2 = torch.argmax(y_train, dim=1) == target_class\n",
     "    accuracy = sum(predictions == y2.detach().numpy().squeeze()) / len(predictions)\n",
     "    print(f'Class {target_class} - Global explanation: \"{global_explanation}\" - Accuracy: {accuracy:.4f}')\n"
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
